---
title: "**ENV222 NOTE**"
subtitle: "*R in statistics (Advanced)*"
author:
  - "TC-tea"
date: "`r Sys.Date()`"
date-format: "YYYY.MM.DD"
output:
  quarto::quarto_document:
    default_output_format: html
    code:
      pandoc_args: ["-V", "lang=en"]
    df_print: paged
toc: true
toc-location: left
fontsize: 12pt
fontfamily: Tahoma
theme: default
comments:
  hypothesis: true
---

<head>
  <!-- Set viewport -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Set styles for light and dark mode -->
  <style>
    mark {
      background-color: yellow;
    }
    .dark-mode {
      background-color: black;
      color: white;
    }
    .light-code {
      color: black;
    }
    .dark-code {
      color: white;
    }
    .quarto-output pre:not([class*="language-"]) {
      background-color: transparent;
      color: inherit;
    }
    .quarto-output pre[class*="language-"] {
      background-color: #F5F5F5;
      color: black;
    }
    .dark-mode .quarto-output pre[class*="language-"] {
      background-color: #2B2B2B;
      color: white;
    }
    /* CSS for back-to-top button */
    #back-to-top {
      position: fixed;
      bottom: 10px;
      right: 22px;
      font-size: 22px;
      border-radius: 50%;
      width: 35px;
      height: 35px;
      text-decoration: none;
    }
    /* CSS for light/dark mode toggle button */
    #myBtn {
      position: fixed;
      top: 20px;
      left: 20px;
      border: none;
      border-radius: 10px;
      padding: 8px;
      background-color: lightgray;
    }
    #myBtn:before {
      content: "â˜€";
    }
    .dark-mode #myBtn:before {
      content: "ğŸŒ™";
    }
  </style>
</head>

<body>
  <!-- Button to toggle light/dark mode -->
  <button id="myBtn" onclick="myFunction()"></button>
  <script>
    function myFunction() {
      var element = document.body;
      element.classList.toggle("dark-mode");
      var codeBlocks = document.querySelectorAll(".quarto-output pre[class*='language-']");
      codeBlocks.forEach(function(block) {
        block.classList.toggle("light-code");
        block.classList.toggle("dark-code");
      });
    }
  </script>
  <!-- Button to scroll back to top of page -->
<a href="#" id="back-to-top" title="Back to top">ğŸš€</a>
</body>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      fig.align = "center")
```

<details>
  <summary>Hyperlink of XJTLU ENV222 courseware</summary>
- [Lecture1 ENV222 Overview](https://pzhao.org/openr/ENV222-Overview.html)<br>
- [Lecture2 R Markdown basic](https://pzhao.org/openr/R-markdown-basic.html)<br>
- [Lecture3 R Markdown advanced](https://pzhao.org/openr/R-markdown-advanced.html)<br>
- [Lecture4 R for characters](https://pzhao.org/openr/R-characters.html)<br>
- [Lecture5 R for time data](https://pzhao.org/openr/R-time.html)<br>
- [Lecture6 Statistical graphs (advanced)](https://pzhao.org/openr/R-graphs-advanced.html)<br>
- [Lecture7 R Tidyverse](https://pzhao.org/openr/R-Tidyverse.html)<br>
- [Lecture8 ANOVA Post-hoc tests](https://pzhao.org/openr/ANOVA-post-hoc.html)<br>
- [Lecture9 MANOVA](https://pzhao.org/openr/MANOVA.html)<br>
- [Lecture10 ANCOVA](https://pzhao.org/openr/ANCOVA.html)<br>
- [Lecture11 MANCOVA](https://pzhao.org/openr/MANCOVA.html)<br>
- [Lecture12 Combining statistics](https://pzhao.org/openr/Combining-statistics.html)<br>
- [Lecture13 Non-parametric hypothesis tests](https://pzhao.org/openr/Non-parametric-hypothesis-tests.html)<br>
- [Lecture14 Multiple linear regression](https://pzhao.org/openr/Multiple-linear-regression.html)<br>
- [Lecture15 Logistic regression](https://pzhao.org/openr/Logistic-regression.html)<br>
- [Lecture16 Poisson regression](https://pzhao.org/openr/Poisson-regression.html)<br>
- [Lecture17 Non-linear regression](https://pzhao.org/openr/Non-linear-regression.html)<br>
- [Lecture18 Principal component analysis](https://pzhao.org/openr/Principal-component-analysis.html)<br>
- [Lecture19 Cluster analysis](https://pzhao.org/openr/Cluster-analysis.html)<br>
</details>

<details>
  <summary>The meaning of each parameter in statistical table (Chinese)</summary>
- Dfï¼ˆè‡ªç”±åº¦ï¼‰: å›å½’è‡ªç”±åº¦ï¼ˆregression degrees of freedomï¼‰å’Œè¯¯å·®è‡ªç”±åº¦ï¼ˆerror degrees of freedomï¼‰çš„æ€»æ•°ï¼Œå…¶ä¸­å›å½’è‡ªç”±åº¦ä¸ºè§£é‡Šå˜é‡çš„ä¸ªæ•°å‡1ï¼Œè¯¯å·®è‡ªç”±åº¦ä¸ºæ ·æœ¬é‡å‡å»è§£é‡Šå˜é‡çš„ä¸ªæ•°ã€‚
- Sum Sqï¼ˆå¹³æ–¹å’Œï¼‰: æ¯ä¸ªæ¥æºçš„å¹³æ–¹å’Œï¼ˆsum of squaresï¼‰ï¼Œæ˜¯å˜é‡ç¦»å‡å·®çš„å¹³æ–¹å’Œã€‚å›å½’å¹³æ–¹å’Œï¼ˆregression sum of squaresï¼‰è¡¨ç¤ºè‡ªå˜é‡å¯¹å› å˜é‡çš„å½±å“ç¨‹åº¦ï¼Œè¯¯å·®å¹³æ–¹å’Œï¼ˆerror sum of squaresï¼‰è¡¨ç¤ºè‡ªå˜é‡æœªèƒ½è§£é‡Šçš„éƒ¨åˆ†ã€‚
- Mean Sqï¼ˆå‡æ–¹ï¼‰: æ¯ä¸ªæ¥æºçš„å‡æ–¹ï¼ˆmean squareï¼‰ï¼Œæ˜¯å¹³æ–¹å’Œé™¤ä»¥è‡ªç”±åº¦å¾—åˆ°çš„å¹³å‡æ•°ã€‚å›å½’å‡æ–¹ï¼ˆregression mean squareï¼‰è¡¨ç¤ºæ¯ä¸ªè‡ªå˜é‡å¯¹å› å˜é‡çš„å½±å“ç¨‹åº¦ï¼Œè¯¯å·®å‡æ–¹ï¼ˆerror mean squareï¼‰è¡¨ç¤ºè‡ªå˜é‡æœªèƒ½è§£é‡Šçš„éƒ¨åˆ†çš„å¹³å‡æ–¹å·®ã€‚
- F value: å›å½’å‡æ–¹ä¸è¯¯å·®å‡æ–¹çš„æ¯”å€¼ï¼Œç”¨äºåˆ¤æ–­æ¨¡å‹çš„æ‹Ÿåˆç¨‹åº¦ï¼ŒFå€¼è¶Šå¤§åˆ™æ¨¡å‹è¶Šå¥½ã€‚åœ¨ä¸€å…ƒçº¿æ€§å›å½’ä¸­ï¼ŒFå€¼ç­‰äºtå€¼çš„å¹³æ–¹ã€‚
- Pr(>F): Probability of obtaining a larger F valueï¼ˆå¾—åˆ°æ›´å¤§çš„Få€¼çš„æ¦‚ç‡ï¼‰Pr(>F)æ˜¯Fæ£€éªŒå¾—åˆ°çš„På€¼ã€‚på€¼è¶Šå°åˆ™è¯´æ˜ç»“æœè¶Šæ˜¾è‘—ï¼Œä¸€èˆ¬å°†æ˜¾è‘—æ€§æ°´å¹³è®¾ä¸º0.05ï¼Œå³å½“på€¼å°äº0.05æ—¶è®¤ä¸ºç»“æœå…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ã€‚
- Pillai: Pillai traceï¼ˆçš®è±è¿¹ï¼‰æ˜¯åœ¨å¤šå…ƒæ–¹å·®åˆ†æä¸­ä½¿ç”¨çš„ä¸€ç§ç»Ÿè®¡é‡ï¼Œç”¨äºè¡¡é‡æ‰€æœ‰å› ç´ å¯¹å› å˜é‡çš„å…±åŒå½±å“ç¨‹åº¦ã€‚
- approx F: F approximationï¼ˆFè¿‘ä¼¼å€¼ï¼‰æ˜¯æ ¹æ®Pillaiè¿¹å€¼è®¡ç®—å‡ºæ¥çš„Få€¼ã€‚å®ƒç”¨äºè¯„ä¼°å¤šå…ƒæ–¹å·®åˆ†æçš„æ€»ä½“æ˜¾è‘—æ€§ã€‚
- num Df: Numerator degrees of freedomï¼ˆåˆ†å­è‡ªç”±åº¦ï¼‰æŒ‡çš„æ˜¯åˆ†å­ä¸­çš„è‡ªç”±åº¦ã€‚
- den Df: Denominator degrees of freedomï¼ˆåˆ†æ¯è‡ªç”±åº¦ï¼‰æŒ‡çš„æ˜¯åˆ†æ¯ä¸­çš„è‡ªç”±åº¦ã€‚
- Residuals: æ®‹å·®ï¼Œæ˜¯æŒ‡å¤šå…ƒæ–¹å·®åˆ†æä¸­çš„è¯¯å·®é¡¹ï¼Œå³ä¸èƒ½è¢«è‡ªå˜é‡è§£é‡Šçš„å› å˜é‡æ–¹å·®ã€‚
- Intercept: æˆªè·ï¼Œä¹Ÿç§°ä¸ºå¸¸æ•°é¡¹ï¼Œè¡¨ç¤ºå½“è‡ªå˜é‡ä¸º0æ—¶ï¼Œå› å˜é‡çš„é¢„æµ‹å€¼ï¼ˆæˆ–æœŸæœ›å€¼ï¼‰ã€‚
- Estimate: å›å½’ç³»æ•°ï¼Œè¡¨ç¤ºè‡ªå˜é‡æ¯å¢åŠ ä¸€ä¸ªå•ä½æ—¶ï¼Œå› å˜é‡å‘ç”Ÿçš„å¹³å‡å˜åŒ–é‡ã€‚
- Std.Error: æ ‡å‡†è¯¯å·®ï¼Œè¡¨ç¤ºä¼°è®¡å€¼çš„ä¸ç¡®å®šæ€§æˆ–è¯¯å·®ï¼Œå³ä¼°è®¡å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„å¹³å‡å·®å¼‚ã€‚
- t value: tå€¼ï¼Œè¡¨ç¤ºå›å½’ç³»æ•°çš„æ˜¾è‘—æ€§ï¼Œå³å›å½’ç³»æ•°é™¤ä»¥å…¶æ ‡å‡†è¯¯å·®ï¼Œå¾—åˆ°çš„å€¼ä¸tåˆ†å¸ƒç›¸æ¯”è¾ƒçš„ç»“æœã€‚
</details>

<details>
  <summary>How to choose between ANOVA, MANOVA, ANCOVA and MANCOVA (Chinese)</summary>
ANOVAã€MANOVAã€ANCOVAå’ŒMANCOVAéƒ½æ˜¯ç»Ÿè®¡å­¦ä¸­å¸¸è§çš„åˆ†ææ–¹æ³•ï¼Œä¸»è¦ç”¨äºæ¯”è¾ƒä¸¤ä¸ªæˆ–å¤šä¸ªç»„ä¹‹é—´çš„å·®å¼‚æ€§ï¼Œå¹¶ç”¨ç»Ÿè®¡å­¦æ–¹æ³•å¯¹è¿™äº›å·®å¼‚è¿›è¡Œæ¨æ–­å’ŒéªŒè¯ã€‚

- ANOVAï¼ˆAnalysis of Varianceï¼‰ï¼šæ–¹å·®åˆ†æï¼Œç”¨äºæ¯”è¾ƒä¸¤ä¸ªæˆ–å¤šä¸ªç»„çš„å‡å€¼æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚ï¼Œé€‚ç”¨äºåªæœ‰ä¸€ä¸ªè‡ªå˜é‡å’Œä¸€ä¸ªå› å˜é‡çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼Œç”¨äºæ¯”è¾ƒä¸åŒæ•™å­¦æ–¹æ³•å¯¹å­¦ç”Ÿæˆç»©çš„å½±å“æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚ã€‚
- MANOVAï¼ˆMultivariate Analysis of Varianceï¼‰ï¼šå¤šå…ƒæ–¹å·®åˆ†æï¼Œç”¨äºæ¯”è¾ƒä¸¤ä¸ªæˆ–å¤šä¸ªç»„çš„å¤šä¸ªç›¸å…³å› å˜é‡æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚ï¼Œé€‚ç”¨äºæœ‰å¤šä¸ªç›¸å…³å› å˜é‡çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼Œç”¨äºæ¯”è¾ƒä¸åŒæ•™å­¦æ–¹æ³•å¯¹å­¦ç”Ÿæˆç»©ã€å­¦ç”Ÿæ€åº¦å’Œå­¦ç”ŸåŠ¨æœºç­‰å¤šä¸ªæ–¹é¢çš„å½±å“æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚ã€‚
- ANCOVAï¼ˆAnalysis of Covarianceï¼‰ï¼šåæ–¹å·®åˆ†æï¼Œç”¨äºæ¯”è¾ƒä¸¤ä¸ªæˆ–å¤šä¸ªç»„çš„å‡å€¼æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚ï¼Œå¹¶æ§åˆ¶ä¸€ä¸ªæˆ–å¤šä¸ªåå˜é‡ï¼ˆå³å½±å“å› ç´ ï¼‰ï¼Œé€‚ç”¨äºéœ€è¦æ§åˆ¶å…¶ä»–å› ç´ å½±å“çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼Œç”¨äºæ¯”è¾ƒä¸åŒæ•™å­¦æ–¹æ³•å¯¹å­¦ç”Ÿæˆç»©çš„å½±å“æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚ï¼ŒåŒæ—¶æ§åˆ¶å­¦ç”Ÿçš„åˆå§‹æ°´å¹³ï¼Œé¿å…å­¦ç”Ÿåˆå§‹æ°´å¹³çš„ä¸åŒå¯¹æ¯”è¾ƒç»“æœäº§ç”Ÿå½±å“ã€‚
- MANCOVAï¼ˆMultivariate Analysis of Covarianceï¼‰ï¼šå¤šå…ƒåæ–¹å·®åˆ†æï¼ŒåŒæ—¶æ§åˆ¶å¤šä¸ªåå˜é‡ï¼Œæ¯”è¾ƒä¸¤ä¸ªæˆ–å¤šä¸ªç»„çš„å¤šä¸ªç›¸å…³å› å˜é‡æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚ï¼Œé€‚ç”¨äºæœ‰å¤šä¸ªç›¸å…³å› å˜é‡å’Œå¤šä¸ªåå˜é‡çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼Œç”¨äºæ¯”è¾ƒä¸åŒæ•™å­¦æ–¹æ³•å¯¹å­¦ç”Ÿæˆç»©ã€å­¦ç”Ÿæ€åº¦å’Œå­¦ç”ŸåŠ¨æœºç­‰å¤šä¸ªæ–¹é¢çš„å½±å“æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚ï¼ŒåŒæ—¶æ§åˆ¶å­¦ç”Ÿçš„åˆå§‹æ°´å¹³ã€æ€§åˆ«ã€å¹´é¾„ç­‰å› ç´ çš„å½±å“ã€‚
</details>

ğŸ‘‰ğŸ»[Click to enter the ENV222 exercise section](../ENV222 exercise/ENV222 exercise.html)<br>
ğŸ‘‰ğŸ»[Click to enter the ENV221 note section](../ENV221 note/ENV221 note.html)

# <span style="color:gray; font-family:Microsoft JhengHei;">**1**</span> **R-markdown**<span style="font-size: 15px;">~(HTML)~</span> **syntax** 

## <span style="color:gray; font-family:Microsoft JhengHei;">**1.1**</span> **Fundamental**

-   Subscript by Rmarkdown: Use `PM~2.5~` to form PM~2.5~. <br>Subscript by html: `log<sub>2</sub>` will be displayed as log<sub>2</sub>.<br>

-   Superscript by Rmarkdown: Use `R^2^` to form R^2^. <br>Superscript by html: `2<sup>n</sup>` will be displayed as 2<sup>n</sup>.<br>

-   Use `$E = mc^2$` to form $E = mc^2$<br>

-   Use `[Link of XJTLU](http://xjtlu.edu.cn)` to form [Link of XJTLU](https://www.xjtlu.edu.cn/en/)<br>

-   Use `<mark>text</mark>` to highlight the <mark>text</mark>

-   Use `<u>text</u>` to add underline to the <u>text</u>

-   Use `<center><img src="images/rstudio-qmd-how-it-works.png" width="1400" height="257"/>` or `<center> ![rstudio qmd how it works](images/rstudio-qmd-how-it-works.png){width=100%}` to form<br><center>![Rstudio qmd how it works](images/rstudio-qmd-how-it-works.png){width="100%"}</center><br>

-   Use `<iframe src="https://www.r-project.org/" width="100%" height="300px"></iframe>` to form a windows which show another file on-line, like this:<iframe src="https://www.r-project.org/" width="100%" height="300px"></iframe>

-   Use the following HTML code to add a video to your Rmarkdown(HTML):
```{html, eval=FALSE}
<video width="420" controls>
  <source src="mov_bbb.mp4" type="video/mp4">
  <source src="mov_bbb.ogg" type="video/ogg">
 Your browser does not support the video tag.
</video>
```

-   Use something like `{r, fig.width = 6, fig.height = 4, fig.align='center'}` in front of the code chunk to change the output graphics

-   Also, use`{r, XXX-Plot, fig.cap="XXX-Plot"}` in the front of code chunk to add a caption of this figure

-   Use something like`<span style="color:red; font-weight:bold; font-size:16px; font-family:Tahoma;">sentence</span>` to change the properties of text

-   Use the following HTML code to add a foldable item to your Rmarkdown(HTML): 
```{html, eval=FALSE}
<details>
  <summary>title</summary>
  content
</details>
```
<details><summary>title</summary>content</details>

-   Use<br> `| Name | Math | English |`<br> `|:----:|:-----|--------:|`<br> `| Tom  | 93   |     100 |`<br> `| Mary | 60   |      90 |`<br> to form<br>

| Name | Math | English |
|:----:|:-----|--------:|
| Tom  | 93   |     100 |
| Mary | 60   |      90 |

-   Or use

```{r echo=TRUE}
library(knitr)
df <- data.frame(
  Math = c(80, 90),
  English = c(85, 95),
  row.names = c("Tom", "Mary")
)
kable(df, format = "markdown")
```

-   Use

```         
- 1. 
- 2. 
    - 1.
    - 2.
- 3.
```

to form sub-rank like this below:<br>

-   

    1.  

-   

    2.  

    -   

        1.  

    -   

        2.  

-   

    3.  

## <span style="color:gray; font-family:Microsoft JhengHei;">**1.2**</span> **Advanced**

-   Numbering, caption, and cross-reference of R-plots in academic paper [Click to see the detail in ENV222 Week5-5.2](https://pzhao.org/openr/R-graphs-advanced.html#numbering-caption-and-cross-reference)

# <span style="color:gray; font-family:Microsoft JhengHei;">**2**</span> **Basic R charaters**

## <span style="color:gray; font-family:Microsoft JhengHei;">**2.1**</span> **Check the data type**

```{r echo=TRUE}
# import dataset
x <- 'The world is on the verge of being flipped topsy-turvy.'
dtf <- read.csv('data/student_names.csv')
head(dtf)
# data type
class(x)
# length of the dataset
length(x)
# length of the sub dataset
nchar(x)
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**2.2**</span> **Index to maximum and minimum values**

-   Find the longest name in [*student_names.csv*]{style="color:green"}, `which.max` or `which.min` is used to find the index of the (first) minimum or maximum of a numeric (or logical) vector

```{r echo=TRUE}
name_n <- nchar(dtf$Name)
name_nmax <- which.max(name_n)
dtf$Name[name_nmax]
# or
dtf$Name[which.max(nchar((dtf$Name)))]
# or
library(magrittr)
dtf$Name %>% nchar() %>% which.max() %>% dtf$Name[.]
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**2.3**</span> **Capital and small letter**

```{r echo=TRUE}
# tolower() toupper()
(xupper <- toupper(x))
(dtf$pro <- tolower(dtf$Prgrm))
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**2.4**</span> **Split string**

```{r echo=TRUE}
# strsplit()
x_word <- strsplit(xupper, ' ')
class(x_word)
# If you want to extract the first element in the list, you need to use double brackets [[]], and if you want to extract the first sublist in the list, use single brackets []
x_word1 <- x_word[[1]]
class(x_word1)
table(x_word1)  # Form a table which involved the frequency of each char acter
x_word1[!duplicated(x_word1)]  # Find the distinct characters in the list by use duplicated() function
unique(x_word1)  # Other way yo detect the distinct characters
lapply(x_word, length)  # The output of the lapply() function is a list
sapply(x_word, length)  # The output of the sapply() function is a vector or a matrix
sapply(x_word, nchar)
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**2.5**</span> **Separate column**

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# separate() is a function in the tidyr package that can be used to split a column in a data box into multiple columns
library(tidyr)
dtf2 <- separate(dtf, Name, c("GivenName", "LastName"), sep = ' ')  # separate(data, col, into, sep)
dtf$FamilyName <- dtf2$LastName
head(dtf)
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**2.6**</span> **Extract**

The world is [on]{style="color:red"} the verge of being flipped topsy-turvy.

```{r echo=TRUE}
# substr() is a built-in function in R that can be used to extract or replace substrings from a character vector
substr(x, 13, 15)  # substr(x, start, stop)
dtf$NameAbb <- substr(dtf$Name, 1, 1)
head(dtf, 3)
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**2.7**</span> **Connect**

```{r echo=TRUE}
# paste() function can convert multiple objects into character vectors and concatenate them
paste(x, '<end>', sep = ' ')  # paste(x1, x2,... sep, collapse)
paste(dtf$NameAbb, '.', sep = '')
paste(dtf$NameAbb, collapse = ' ')  # collapse = ' ' put all of the characters into a character
paste(dtf$NameAbb, dtf$FamilyName, sep = '. ')[7]  # This is my name for academic essay cite
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**2.8**</span> **Find**

```{r echo=TRUE}
# grep() function in R is a built-in function that searches for a pattern match in each element of character 
y <- c("R", "Python", "Java")
grep("Java", y)
for(i in 1:length(y)) {
  print(grep(as.character(y[i]), y))
}
sapply(y, function(x) grep(x, y))

head(table(dtf2$GivenName), 12)
grep('Jiayi', dtf$Name, value = TRUE)
grep('Jiayi|Guo', dtf$Name, value = TRUE)

# regexpr() function is used to identify the position of the pattern in the character vector, where each element is searched separately.
z <- c("R is fun", "R is cool", "R is awesome")
regexpr("is", z)  # Returns include starting position, duration length, data type ...
gregexpr("is", z)  # The gregexpr() function returns all matching positions and lengths, as a list
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**2.9**</span> **Replace**

```{r echo=TRUE}
# gsub()
gsub(' ', '-', x)
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**2.10**</span> **[Regular expression]{style="color:orange"}**

```{r echo=TRUE}
# help(regex)
# Find the one who has a given name with 4 letters and a family name with 4 letters
grep('^[[:alpha:]]{4} [[:alpha:]]{4}$', dtf$Name, value = TRUE)

# Here, parentheses are used to create a capturing group. A capturing group is a subexpression of a regular expression that can capture and store the matched text during matching. 
# In this example, the capturing group is used to extract the first word from the string. Without a capturing group, the entire matched string would be replaced with \\1 instead of just the first word.
dtf$FirstName <- gsub('^([^ ]+).+[^ ]+$', '\\1', dtf$Name)
head(dtf)
```

```{r eval=FALSE}
Rmarkdown ä¸­æ­£åˆ™è¡¨è¾¾å¼çš„åŸºæœ¬è¯­æ³•å¦‚ä¸‹ï¼š
  . åŒ¹é…ä»»æ„å•ä¸ªå­—ç¬¦ï¼Œé™¤äº†æ¢è¡Œç¬¦ã€‚
  [ ] åŒ¹é…æ–¹æ‹¬å·å†…çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦ï¼Œä¾‹å¦‚ [abc] åŒ¹é… a æˆ– b æˆ– cã€‚
  [^ ] åŒ¹é…æ–¹æ‹¬å·å¤–çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦ï¼Œä¾‹å¦‚ [^abc] åŒ¹é…é™¤äº† a å’Œ b å’Œ c ä¹‹å¤–çš„ä»»æ„å­—ç¬¦ã€‚
  - åœ¨æ–¹æ‹¬å·å†…è¡¨ç¤ºèŒƒå›´ï¼Œä¾‹å¦‚ [a-z] åŒ¹é…å°å†™å­—æ¯ï¼Œ [0-9] åŒ¹é…æ•°å­—ã€‚
  \d \D \w \W \s \S åˆ†åˆ«åŒ¹é…æ•°å­—ã€éæ•°å­—ã€å•è¯å­—ç¬¦ï¼ˆå­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿ï¼‰ã€éå•è¯å­—ç¬¦ã€ç©ºç™½ç¬¦ï¼ˆç©ºæ ¼ã€åˆ¶è¡¨ç¬¦å’Œæ¢è¡Œç¬¦ï¼‰ã€éç©ºç™½ç¬¦ã€‚
  \b \B ^ $ \ åˆ†åˆ«åŒ¹é…å•è¯è¾¹ç•Œï¼ˆå•è¯å’Œéå•è¯ä¹‹é—´ï¼‰ã€éå•è¯è¾¹ç•Œï¼ˆä¸¤ä¸ªå•è¯æˆ–ä¸¤ä¸ªéå•è¯ä¹‹é—´ï¼‰ã€å­—ç¬¦ä¸²å¼€å¤´ã€å­—ç¬¦ä¸²ç»“å°¾ã€è½¬ä¹‰ç¬¦ï¼ˆç”¨äºåŒ¹é…å…ƒå­—ç¬¦æœ¬èº«ï¼‰ã€‚
  ( ) | ? + * { } \ åˆ†åˆ«åŒ¹é…åˆ†ç»„æˆ–æ•è·å­è¡¨è¾¾å¼ï¼ˆå¯ä»¥ç”¨åæ–œæ åŠ æ•°å­—å¼•ç”¨ï¼‰ï¼Œé€‰æ‹©ï¼ˆåŒ¹é…å·¦è¾¹æˆ–å³è¾¹ï¼‰ï¼Œé›¶æ¬¡æˆ–ä¸€æ¬¡é‡å¤ï¼Œä¸€æ¬¡æˆ–å¤šæ¬¡é‡å¤ï¼Œé›¶æ¬¡æˆ–å¤šæ¬¡é‡å¤ï¼ŒæŒ‡å®šé‡å¤æ¬¡æ•°ï¼Œé›¶å®½æ–­è¨€ï¼ˆåŒ¹é…ä½ç½®è€Œä¸æ˜¯å­—ç¬¦ï¼‰ã€‚
ç®€å•çš„ä¾‹å­ï¼ŒæŸ¥æ‰¾ Markdown é“¾æ¥ï¼ˆ[This is a link](https://www.example.com)ï¼‰ï¼š
\[([^\]]+)\]\(([^)]+)\)
è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼å¯ä»¥åˆ†è§£ä¸ºä»¥ä¸‹éƒ¨åˆ†ï¼š
  \[ åŒ¹é…å·¦æ–¹æ‹¬å·
  ([^\]]+) åŒ¹é…å¹¶æ•è·ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ˜¯å³æ–¹æ‹¬å·çš„å­—ç¬¦
  \] åŒ¹é…å³æ–¹æ‹¬å·
  \( åŒ¹é…å·¦åœ†æ‹¬å·
  ([^)]+) åŒ¹é…å¹¶æ•è·ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ˜¯å³åœ†æ‹¬å·çš„å­—ç¬¦
  \) åŒ¹é…å³åœ†æ‹¬å·
```

# <span style="color:gray; font-family:Microsoft JhengHei;">**3**</span> **Time data in R**

## <span style="color:gray; font-family:Microsoft JhengHei;">**3.1**</span> **Format of time**

```{r echo=TRUE}
# Check the current date
date()
# character
d1 <- "2/11/1962"
# Date/Time format, we can just directly use like "d2 + 1" to add 1 day to d2
d2 <- Sys.Date()
t2 <- Sys.time()
# Check their type
t(list(class(d1), class(d2), class(t2)))
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**3.2**</span> **Numeric of date**

```{r echo=TRUE}
# Use format="" to identify the character to date
d3 <- as.Date("2/11/1962", format="%d/%m/%Y" )
as.numeric(d3)
d3 + 2617
format(d3, '%Y %m %d')
format(d3, "%Y %B %d %A")
# Different format will have different meaning
d4 <- as.Date( "2/11/1962", format="%m/%d/%Y" )
d3 == d4
```

### <span style="color:gray; font-family:Microsoft JhengHei;">**3.2.1**</span> **Time format codes**

`%Y`: Four-digit year<br> `%y`: Two-digit year<br> `%m`: Two-digit month (01\~12)<br> `%d`: Two-digit day of the month (01\~31)<br> `%H`: Hour in 24-hour format (00\~23)<br> `%M`: Two-digit minute (00\~59)<br> `%S`: Two-digit second (00\~59)<br> `%z`: Time zone offset, for example +0800<br> `%Z`: Time zone name, for example CST

## <span style="color:gray; font-family:Microsoft JhengHei;">**3.3**</span> **Calculating date**

```{r echo=TRUE}
# import built-in data diet (The data concern a subsample of subjects drawn from larger cohort studies of the incidence of coronary heart disease (CHD))
library('Epi')
data("diet")
str(diet)
# Prepare data which we will deal with
bdat <- diet$dox[1]
bdat
# Some basic calculation between dates
bdat + 1

diet$dox2 <- format(diet$dox, format="%A %d %B %Y")
head(diet$dox2, 3)

# Some advanced calculation between dates
max(diet$dox)
range(diet$dox)
mean(diet$dox)
median(diet$dox)
diff(range(diet$dox))
difftime(min(diet$dox), max(diet$dox), units = "weeks")  # Set unit

# Epi::cal.yr() function converts the date format to numeric format
diet2 <- Epi::cal.yr(diet)
str(diet2)
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**3.4**</span> **Set time zone & calculation**

```{r echo=TRUE}
bd <- '1994-09-22 20:30:00'
class(bd)
bdtime <- strptime(x = bd, format = '%Y-%m-%d %H:%M:%S', tz = "Asia/Shanghai")  # Set character to time format and add a time zone
class(bdtime)
t(unclass(bdtime))
bdtime$wday
format(bdtime, format = '%d.%m.%Y')
bdtime + 1
# Also, some essential calculation
bd2 <- '1995-09-01 7:30:00'
bdtime2 <- strptime(bd2, format = '%Y-%m-%d %H:%M:%S', tz = 'Asia/Shanghai')
bdtime2 - bdtime
difftime(time1 = bdtime2, time2 = bdtime, units = 'secs')  # Set unit
mean(c(bdtime, bdtime2))
```

# <span style="color:gray; font-family:Microsoft JhengHei;">**4**</span> **[LaTeX]{style="color:orange"}**

## <span style="color:gray; font-family:Microsoft JhengHei;">**4.1**</span> **Fundamental**

<center><img src="images/Mathematical Annotation in R.png" width="600" height="579"/></center>

-   Use `$$e^{i\pi}+1=0$$` to form Euler's Law expression $$e^{i\pi}+1=0$$
-   [Hyperlink of a CN website for more detail about LaTeX](https://blog.csdn.net/ViatorSun/article/details/82826664)

## <span style="color:gray; font-family:Microsoft JhengHei;">**4.2**</span> **Advanced**

-   Here are some additional formulas from ENV221 statistic method:<br>
    1.  [Z-test]{style="color:orange"}:<br> The LaTex expression for Z-test is: $$Z=\frac{\overline{x}-\mu}{\frac{\sigma}{\sqrt{n}}}$$ where $\overline{x}$ is the sample mean, $\mu$ is the population mean, $\sigma$ is the population standard deviation, and $n$ is the sample size.
    2.  [t-test]{style="color:orange"}:<br> The LaTex expression for t-test is: $$t=\frac{\overline{x}-\mu}{\frac{s}{\sqrt{n}}}$$ where $\overline{x}$ is the sample mean, $\mu$ is the population mean, $s$ is the sample standard deviation, and $n$ is the sample size.
    3.  [F-test]{style="color:orange"}:<br> The LaTex expression for F-test is: $$F=\frac{s_1^2}{s_2^2}$$ where $s_1^2$ is the variance of the first sample and $s_2^2$ is the variance of the second sample.
    4.  [Chi-square test]{style="color:orange"}:<br> The LaTex expression for the chi-square test is: $$\chi2=\sum_{i=1}{n}\frac{(O_i-E_i)^2}{E_i}$$ where $O_i$ represents observed values and $E_i$ represents expected values.

# <span style="color:gray; font-family:Microsoft JhengHei;">**5**</span> **R graph (advanced)**

## <span style="color:gray; font-family:Microsoft JhengHei;">**5.1**</span> **Different theme of plot**

```{r, fig.align='center'}
library(ggplot2)
bw <- ggplot(CO2) + geom_point(aes(conc, uptake)) + theme_bw()
test <- ggplot(CO2) + geom_point(aes(conc, uptake)) + theme_test()
classic <- ggplot(CO2) + geom_point(aes(conc, uptake)) + theme_classic()
library(patchwork)
bw + test + classic + 
  plot_layout(ncol = 3, widths = c(1, 1, 1), heights = c(1, 1, 1)) + 
  plot_annotation(
    title = expression(CO[2] * " uptake by plant type plot with different theme"),
    tag_levels = "A"
  )
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**5.2**</span> **Math formulas with R**

```{r, fig.width = 8, fig.height = 4, fig.align='center'}
head(CO2)
# fundamental expression
plot(CO2$conc, CO2$uptake, pch = 16, las = 1, 
     xlab = 'CO2 concentration', ylab = 'CO2 uptake')
# Advanced expression (Use `?plotmath` to check more details of mathematical annotation in R)
plot(CO2$conc, CO2$uptake, pch = 16, las = 1, 
     xlab = expression('CO'[2] * ' concentration (mL/L)'), 
     ylab = expression('CO'[2] * ' uptake (' *mu * 'mol m'^-2 * 's'^-1 * ')'))
# LaTeX expression
library(latex2exp)
plot(CO2$conc, CO2$uptake, pch = 16, las = 1, 
     xlab = TeX('CO$_2$ concentration (mL/L)'), 
     ylab = TeX('CO$_2$ uptake ($\\mu$mol m$^{-2}$ s$^{-1}$)'))
text(850, 30, expression(prod(plain(P)(X == x), x)))
```

-   [Hyperlink of a CN website for more detail about Mathematical Annotation in R](https://www.cnblogs.com/kisen/p/12574830.html)

## <span style="color:gray; font-family:Microsoft JhengHei;">**5.3**</span> **Size and layout**

-   ggplot2: patchwork package is used to range the size and layout of multiply plots

```{r, fig.width = 8, fig.height = 5, fig.align='center', warning=FALSE}
library(patchwork)
p1 <- ggplot(airquality) + geom_boxplot(aes(as.factor(Month), Ozone))
p2 <- ggplot(airquality) + geom_point(aes(Solar.R, Ozone))
p3 <- ggplot(airquality) + geom_histogram(aes(Ozone))
p1 + p2 + p3
p1 + p2 / p3
(p1 + p2) / p3
(p1 + p2) / p3 + plot_annotation(tag_levels = 'A') + 
  plot_layout(ncol = 2, widths = c(1, 1), heights = c(1, 1))  # plot_layout() function is used to define the grid layout of the composite graph.
```

-   Built-in par() function

```{r}
par(mfrow = c(2, 3))  # Set the layout by using vector c(x, y)
plot(airquality$Solar.R, airquality$Ozone)
hist(airquality$Solar.R)
barplot(airquality$Month)
plot(airquality$Solar.R, airquality$Ozone)
hist(airquality$Solar.R)
barplot(airquality$Month)
```

-   <span style="color:red">Built-in layout() function</span>

```{r, fig.width = 8, fig.height = 5, fig.align='center'}
# Use a matrix to store the information about layout
mymat <- matrix(1:6, nrow = 2)
layout(mymat)
plot(airquality$Solar.R, airquality$Ozone)
hist(airquality$Solar.R)
barplot(airquality$Month)
plot(airquality$Solar.R, airquality$Ozone)
hist(airquality$Solar.R)
barplot(airquality$Month)
# Also, customize the exact layout by using some parameters like 'widths=' and 'heights=' by filling vector
mymat <- matrix(c(1, 1:5), nrow = 2)
mymat  # Check the matrix which was used to layout plots
layout(mymat, widths = c(1, 1, 2), heights = c(1, 2))
plot(airquality$Solar.R, airquality$Ozone)
hist(airquality$Solar.R)
barplot(airquality$Month)
plot(airquality$Solar.R, airquality$Ozone)
hist(airquality$Solar.R)
# This is an example from quiz1. Also, please check the exercises to view more difficult questions
mymat <- matrix(c(1, 2, 3, 0), nrow = 2)
mymat  # Check the matrix which was used to layout plots
layout(mymat, widths = c(4, 1), heights = c(2, 1))  # Set the ratio between widths and heights
plot(iris$Sepal.Length, iris$Sepal.Width, pch=20, xlab='Sepal Length (cm)', ylab='Sepal Width (cm)', las=1)
boxplot(iris$Sepal.Length, pch=20, las=1, horizontal=T)
boxplot(iris$Sepal.Width, pch=20, las=2)
```

# <span style="color:gray; font-family:Microsoft JhengHei;">**6**</span> **R Tidyverse**

## <span style="color:gray; font-family:Microsoft JhengHei;">**6.1**</span> **Workflow**

<center>![Tidyverse workflow](images/Tidyverse workflow.png){width=75%}</center>

## <span style="color:gray; font-family:Microsoft JhengHei;">**6.2**</span> **Fundamental operations**
```{r, message=FALSE}
# Load the package
library(tidyverse)
# Check the members of them
tidyverse_packages()
```
Core members and their function:

- `ggplot2`: Creating graphics
- `dplyr`: Data manipulation
- `tidyr`: Get to tidy data
- `readr`: Read rectangular data
- `purrr`: Functional programming
- `tibble`: Re-imagining of the data frame
- `stringr`: Working with strings
- `forcats`: Working with factors

## <span style="color:gray; font-family:Microsoft JhengHei;">**6.3**</span> **Pipe operator**

The pipe operator can be written as `%>%` or `|>`
```{r}
x <- c(0.109, 0.359, 0.63, 0.996, 0.515, 0.142, 0.017, 0.829, 0.907)
# Method 1:
y1 <- log(x)
y2 <- diff(y1)
y3 <- exp(y2)
z <- round(y3)
# Method 2
z <- round(exp(diff(log(x))))
# Pipe method
z <- x %>% log() %>% diff() %>% exp() %>% round()
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**6.4**</span> **Mutiply work by using tidyverse pipe**

### <span style="color:gray; font-family:Microsoft JhengHei;">**6.4.1**</span> **Graph work**
```{r, fig.align='center'}
# By using R built-in par() function and a loop
par(mfrow = c(2, 2))
for (i in 1:4) {
  boxplot(iris[, i] ~ iris$Species, las = 1, xlab = 'Species', ylab = names(iris)[i])
}
# By using pivot_longer() function and tidyverse pipe
iris |> pivot_longer(-Species) |> ggplot() + geom_boxplot(aes(Species, value)) + facet_wrap(name ~.)
```
### <span style="color:gray; font-family:Microsoft JhengHei;">**6.4.2**</span> **Statistic work**
```{r}
# base R
dtf1_mean <- data.frame(Species = unique(iris$Species), Mean_Sepal_Length = tapply(iris$Sepal.Length, iris$Species, mean, na.rm = TRUE))
dtf1_sd <- data.frame(Species = unique(iris$Species), SD_Sepal_Length = tapply(iris$Sepal.Length, iris$Species, sd, na.rm = TRUE))
dtf1_median <- data.frame(Species = unique(iris$Species), Median_Sepal_Length = tapply(iris$Sepal.Length, iris$Species, median, na.rm = TRUE))
names(dtf1_mean) <- c("Species", "Mean_Sepal_Length")
names(dtf1_sd) <- c("Species", "SD_Sepal_Length")
names(dtf1_median) <- c("Species", "Median_Sepal_Length")
cbind(dtf1_mean, dtf1_sd, dtf1_median)  # Show them in one table
# use a loop
dtf <- data.frame(rep(NA, 3))
for (i in 1:4) {
  dtf1_mean <- data.frame(tapply(iris[, i], iris$Species, mean, na.rm = TRUE))
  dtf1_sd <- data.frame(tapply(iris[, i], iris$Species, sd, na.rm = TRUE))
  dtf1_median <- data.frame(tapply(iris[, i], iris$Species, median, na.rm = TRUE))
  dtf1 <- cbind(dtf1_mean, dtf1_sd, dtf1_median)
  names(dtf1) <- paste0(names(iris)[i], '.', c('mean', 'sd', 'median'))
  dtf <- cbind(dtf, dtf1)
}
dtf
# tidyverse
dtf <- iris |> 
  pivot_longer(-Species) |> 
  group_by(Species, name) |> 
  summarise(mean = mean(value, na.rm = TRUE),
            sd   = sd(value, na.rm = TRUE),
            median = median(value, na.rm = TRUE),
            .groups = "drop")

dtf
```



## <span style="color:gray; font-family:Microsoft JhengHei;">**6.5**</span> **Tidy the dataset**

```{r}
# Original dataset of table1
table1
# Compute rate per 10,000
table1 %>% mutate(rate = cases / population * 10000)
# Compute cases per year
table1 %>% count(year, wt = cases)
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**6.6**</span> **Conversions the dataframe type**

```{r}
# Original dataset of table2
table2
# Divided the type into cases and population
table2 %>% pivot_wider(names_from = type, values_from = count)
# Original dataset of table3
table3
# Separate the rate into cases and population
table3 %>% separate(col = rate, into = c("cases", "population"), sep = "/")
# Original dataset of table4a and table4b
cbind(table4a, table4b)
# Put table4a and table4b together to form a new table with both of their dataset
tidy4a_changed <- table4a %>% pivot_longer(c(`1999`, `2000`), names_to = "year", values_to = "cases")
tidy4b_changed <- table4b %>% pivot_longer(c(`1999`, `2000`), names_to = "year", values_to = "population")
left_join(tidy4a_changed, tidy4b_changed)   ## Kind of like MySQL
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**6.7**</span> **Find missing observations**

```{r}
library(openair)
library(tidyverse)
# create a function to count missing observations
sum_of_na <- function(x){
  sum(is.na(x))
}
mydata %>% summarise(
  across(everything(), sum_of_na)
)
```

# <span style="color:gray; font-family:Microsoft JhengHei;">**7**</span> **ANOVA (Post-hoc tests)**

## <span style="color:gray; font-family:Microsoft JhengHei;">**7.1**</span> **Post-hoc tests**

Background informations: A biologist studies the weight gain of male lab rats on diets over a 4-week period. Three different diets are applied.

```{r}
# Statistic anlysis
(dtf <- data.frame(diet1 = c(90, 95, 100),
                   diet2 = c(120, 125, 130),
                   diet3 = c(125, 130, 135)))
dtf2 <- stack(dtf)
names(dtf2) <- c("wg", "diet")
wg_aov <- aov(wg ~ diet, data = dtf2)
summary(wg_aov)
```

```{r, fig.width = 6, fig.height = 4, fig.align='center'}
# Visualization
library(ggplot2)
ggplot(dtf2) + geom_boxplot(aes(wg, diet))
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**7.2**</span> **Fisherâ€™s Least Significant Difference (LSD) Test**

### <span style="color:gray; font-family:Microsoft JhengHei;">**7.2.1**</span> **Concept**

Pair-wise comparisons of all the groups based on the $t$-test:
$$L S D=t_{\alpha / 2} \sqrt{S_{p}^{2}\left(\frac{1}{n_1}+\frac{1}{n_2}+\cdots\right)}$$
$$S_{p}^{2}=\frac{\left(n_{1}-1\right) S_{1}^{2}+\left(n_{2}-1\right) S_{2}^{2}+\left(n_{3}-1\right) S_{3}^{2}+\cdots}{\left(n_{1}-1\right)+\left(n_{2}-1\right)+\left(n_{3}-1\right)+\cdots}$$

- $S_{p}^{2}:$: pooled standard deviation (some use Mean Standard Error)
- $t_{\alpha / 2}: \mathrm{t}$: $t$ critical value at $\alpha=0.025$
- Degree of freedom: $N - k$
    - $N$: total observations
    - $k$: number of factors
- If $\left|\bar{x}_{1}-\bar{x}_{2}\right|>L S D$, then the difference of $x_1$ group and $x_2$ group is significant at $\alpha$.
- In multiple comparisons ($k$ factors), the number of comparison needed is: $\frac{k(k-1)}{2}$

### <span style="color:gray; font-family:Microsoft JhengHei;">**7.2.2**</span> **Example**
(Rats on diets in the previous section)

1. Step by step
```{r}
# Calculate LSD
n <- nrow(dtf2)
k <- nlevels(dtf2$diet)
dfree <- n - k
t_critical <- qt(0.05/2, df = dfree, lower.tail = FALSE)
sp2 <- sum((3 - 1) * apply(dtf, 2, sd) ^ 2)/ dfree
LSD <- t_critical * sqrt(sp2 * (1/3 + 1/3 + 1/3))
# Calculate |mean_x1-mean_x2|
dtf_groupmean <- colMeans(dtf)
paired_groupmean <- combn(dtf_groupmean, 2)
paired_groupmean[2, ] - paired_groupmean[1, ]
```
2. Step by step by using loop
```{r}
library(dplyr)
dtf_sm <- dtf2 |> 
  group_by(diet) |> 
  summarise(n = length(wg), sd = sd(wg), mean = mean(wg))
sp2 <- sum((dtf_sm$n - 1) * dtf_sm$sd ^ 2 )/ dfree
LSD <- t_critical * sqrt(sp2 * sum(1 / dtf_sm$n))
paired_groupmean <- combn(dtf_sm$mean, 2)
paired_groupmean[2, ] - paired_groupmean[1, ]
```
3. One step
```{r}
library(agricolae)
# Statistic analysis
LSD.test(wg_aov, "diet", p.adj = "bonferroni") |> print()
```
<details>
  <summary>The meaning of each parameter in this table</summary>
- $statisticsï¼šåŒ…å«ANOVAåˆ†æçš„ç»Ÿè®¡é‡ï¼ˆstatisticsï¼‰çš„åˆ—è¡¨ã€‚
   - MSerrorï¼šå¹³å‡æ–¹å·®è¯¯å·®ï¼ˆmean square errorï¼‰ï¼Œä¹Ÿç§°ä¸ºæ®‹å·®æ–¹å·®ï¼Œè¡¨ç¤ºæ¨¡å‹è¯¯å·®çš„å¹³å‡ç¨‹åº¦ã€‚
   - Dfï¼šè‡ªç”±åº¦ï¼ˆdegrees of freedomï¼‰ã€‚
   - Meanï¼šå‡å€¼ï¼ˆmeanï¼‰ã€‚
   - CVï¼šå˜å¼‚ç³»æ•°ï¼ˆcoefficient of variationï¼‰ï¼Œå˜å¼‚ç³»æ•°è¶Šå¤§ï¼Œè¯´æ˜æ•°æ®çš„ç¦»æ•£ç¨‹åº¦è¶Šå¤§ã€‚
   - t.valueï¼štå€¼ï¼ˆt-valueï¼‰ï¼Œè¡¨ç¤ºç»„é—´å‡å€¼ä¹‹é—´çš„æ˜¾è‘—æ€§å·®å¼‚ç¨‹åº¦ã€‚
   - MSDï¼šæœ€å°æ˜¾è‘—å·®å¼‚ï¼ˆminimum significant differenceï¼‰ï¼Œè¡¨ç¤ºåœ¨æ˜¾è‘—æ€§æ°´å¹³ä¸‹ä¸¤ä¸ªç»„ä¹‹é—´çš„æœ€å°æ˜¾è‘—å·®å¼‚å€¼ã€‚
- $parametersï¼šåŒ…å«å¯¹æ¯”åˆ†æçš„å‚æ•°ï¼ˆparametersï¼‰çš„åˆ—è¡¨ã€‚
   - testï¼šæ‰€ä½¿ç”¨çš„å¤šé‡æ¯”è¾ƒæ–¹æ³•ï¼ˆmultiple comparison methodï¼‰ï¼Œæ­¤å¤„ä¸ºFisher-LSDæ³•ã€‚
   - p.ajustedï¼šç»è¿‡æ ¡æ­£åçš„æ˜¾è‘—æ€§æ°´å¹³ï¼ˆadjusted significance levelï¼‰ï¼Œæ­¤å¤„ä¸ºBonferroniæ³•ã€‚
   -name.tï¼šæ‰€è¿›è¡Œå¯¹æ¯”åˆ†æçš„å› ç´ åç§°ï¼ˆname of tested factorï¼‰ï¼Œæ­¤å¤„ä¸ºdietã€‚
   - ntrï¼šå› ç´ æ°´å¹³æ•°ï¼ˆnumber of treatmentsï¼‰ï¼Œæ­¤å¤„ä¸º3ã€‚
   - alphaï¼šæ˜¾è‘—æ€§æ°´å¹³ï¼ˆsignificance levelï¼‰ï¼Œæ­¤å¤„ä¸º0.05ã€‚
- $meansï¼šåŒ…å«å„ç»„å‡å€¼å’Œç»Ÿè®¡ä¿¡æ¯ï¼ˆmeans and statisticsï¼‰çš„åˆ—è¡¨ã€‚
   - wgï¼šç»„å‡å€¼ï¼ˆgroup meanï¼‰ã€‚
   - stdï¼šæ ‡å‡†å·®ï¼ˆstandard deviationï¼‰ã€‚
   - rï¼šé‡å¤æ¬¡æ•°ï¼ˆreplicationsï¼‰ã€‚
   - LCLï¼šä¸‹é™ç½®ä¿¡åŒºé—´ï¼ˆlower confidence limitï¼‰ã€‚
   - UCLï¼šä¸Šé™ç½®ä¿¡åŒºé—´ï¼ˆupper confidence limitï¼‰ã€‚
   - Minï¼šæœ€å°å€¼ï¼ˆminimum valueï¼‰ã€‚
   - Maxï¼šæœ€å¤§å€¼ï¼ˆmaximum valueï¼‰ã€‚
   - Q25ï¼š25%åˆ†ä½æ•°ï¼ˆ25th percentileï¼‰ã€‚
   - Q50ï¼š50%åˆ†ä½æ•°ï¼ˆ50th percentileï¼‰ï¼Œä¹Ÿç§°ä¸ºä¸­ä½æ•°ã€‚
   - Q75ï¼š75%åˆ†ä½æ•°ï¼ˆ75th percentileï¼‰ã€‚
- $comparisonï¼šå¯¹æ¯”åˆ†æç»“æœï¼ˆcomparisonï¼‰ï¼Œæ­¤å¤„ä¸ºç©ºã€‚
- $groupsï¼šåˆ†ç»„ç»“æœï¼ˆgroupsï¼‰ã€‚
   - wgï¼šç»„å‡å€¼ï¼ˆgroup meanï¼‰ã€‚
   - groupsï¼šåˆ†ç»„ç»“æœï¼ˆgroupsï¼‰ï¼Œä½¿ç”¨å­—æ¯è¡¨ç¤ºä¸åŒç»„åˆ«ï¼Œç›¸åŒå­—æ¯è¡¨ç¤ºåœ¨ç»Ÿè®¡ä¸Šæ²¡æœ‰æ˜¾è‘—å·®å¼‚ã€‚
- attr(,"class")ï¼šç»“æœçš„ç±»åˆ«ï¼ˆclassï¼‰ï¼Œæ­¤å¤„ä¸º"group"ã€‚
</details>
```{r, fig.width = 6, fig.height = 4, fig.align='center'}
# Visualization
LSD.test(wg_aov, "diet", p.adj = "bonferroni") |> plot()
box()
```
Conclusion: At $\alpha = 0.05$, Diet 2 and Diet 3 are significantly different from Diet 1 in the mean weight gain, while Diet 2 is not significantly different from Diet 3.

## <span style="color:gray; font-family:Microsoft JhengHei;">**7.3**</span> **Bonferroni t-test**

### <span style="color:gray; font-family:Microsoft JhengHei;">**7.3.1**</span> **Concept**

A multiple-comparison post-hoc test, which sets the significance cut off at $\alpha/m$ for each comparison, where $m$ represents [the <u>number</u> of comparisons we apply]{style="color:red"}.

[Overall chance of making a Type I error:]{style="color:#880000"}
```{r}
m <- 1:100
siglevel <- 0.05
Type_I <- 1 - (1 - (siglevel / m)) ^ m
Type_I
```

### <span style="color:gray; font-family:Microsoft JhengHei;">**7.3.2**</span> **Example**
(Rats on diets in the previous section)

1. Step by step
```{r}
m <- choose(nlevels(dtf2$diet), 2)  # 1:2 or 1:3 or 2:3
alpha_cor <- 0.05 / m
```
```{r}
# Pairwise comparison between diet1 and diet2
t.test(wg ~ diet, dtf2, subset = diet %in% c("diet1", "diet2"), conf.level = 1 - alpha_cor)
# Pairwise comparison between diet1 and diet3
t.test(wg ~ diet, dtf2, subset = diet %in% c("diet1", "diet3"), conf.level = 1 - alpha_cor)
# Pairwise comparison between diet2 and diet3
t.test(wg ~ diet, dtf2, subset = diet %in% c("diet2", "diet3"), conf.level = 1 - alpha_cor)
```
2. One step
```{r}
(diet_pt <- pairwise.t.test(dtf2$wg, dtf2$diet, pool.sd = FALSE,var.equal = TRUE, p.adj = "none"))
diet_pt$p.value < 0.05
```
Conclusion: At $\alpha = 0.05$, Diet 2 and Diet 3 are significantly different from Diet 1 in the mean weight gain, while Diet 2 is not significantly different from Diet 3.

# <span style="color:gray; font-family:Microsoft JhengHei;">**8**</span> **MANOVA**
<center>![One-Way ANOVA (ENV221)](images/One-Way ANOVA.jpg)</center>
<center>![Two-Way ANOVA (ENV221)](images/Two-Way ANOVA.jpg)</center>
<center>[The Differences Between ANOVA, ANCOVA, MANOVA, and MANCOVA](https://www.statology.org/differences-between-anova-ancova-manova-mancova/)</center>

## <span style="color:gray; font-family:Microsoft JhengHei;">**8.1**</span> **Definition of MANOVA**

**Univariate Analysis of Variance (ANOVA):**

- one dependent variable (continuous) ~ one or multiple independent variables (categorical).  
**Multivariate Analysis of Variance (MANOVA)**
- multiple dependent variables (continuous) ~ one or multiple independent variables (categorical).  
Comparing multivariate sample means. It uses the covariance between outcome variables in testing the statistical significance of the mean differences when there are multiple dependent variables.  

Merit of MANOVA:

1. Reduce the Type I error
2. It allows for the analysis of multiple dependent variables simultaneously
3. It provides information about the strength and direction of relationships

## <span style="color:gray; font-family:Microsoft JhengHei;">**8.2**</span> **Coding and visualization**

**Example:** Influence of <u>teaching methods</u> on student <u>satisfaction scores</u> and <u>exam scores</u>.
```{r, fig.width = 6, fig.height = 4, fig.align='center', message=FALSE}
dtf <- read.csv('data/teaching_methods.csv')
head(dtf, 3)
# ANOVA between Test and Method
summary(aov(Test ~ Method, data = dtf))
# ANOVA between Satisfaction and Method
summary(aov(Satisfaction ~ Method, data = dtf))
# Visualization with Scatter plot
library(ggplot2)
library(tidyr)
dtf |> pivot_longer(-Method) |> 
  ggplot() + 
  geom_dotplot(aes(x = Method, y = value, group = Method), binaxis = "y", stackdir = "center") + 
  facet_wrap(name~.)
# Visualization with Box plot
par(mfrow = c(1, 3))
boxplot(Test ~ Method, data = dtf)
boxplot(Satisfaction ~ Method, data = dtf)
plot(dtf$Satisfaction, dtf$Test, col = dtf$Method, pch = 16, xlab = 'Satisfaction', ylab = 'Test')
# MANOVA method: use manova() function with multiple response variables ~ one or multiple factor
# column bind way
tm_manova <- manova(cbind(dtf$Test, dtf$Satisfaction) ~ dtf$Method)
# matrix way
tm_manova <- manova(as.matrix(dtf[, c('Test', 'Satisfaction')]) ~ dtf$Method)
summary(tm_manova)
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**8.3**</span> **One-way MANOVA**

<center>![One-way MANOVA](images/One-way MANOVA.jpg)</center>
**Example:** The iris dataset. Do the species have influence on the sepal size?
```{r, fig.align='center', message=FALSE}
# Visualization
library(ggplot2)
library(tidyr)
iris[, c('Species', 'Sepal.Length', 'Sepal.Width')] |> 
  pivot_longer(cols = c(Sepal.Length, Sepal.Width)) |> 
  ggplot() +
  geom_boxplot(aes(Species, value, fill = name)) +
  labs(y = 'Size (cm)', fill = '')
library(gplots)
par(mfrow = c(1, 2))
plotmeans(iris$Sepal.Length ~ iris$Species, xlab = "Species", ylab = "Sepal length")
plotmeans(iris$Sepal.Width ~ iris$Species, xlab = "Species", ylab = "Sepal width")
```
**Hypothesis:** multivariate normality test

- $H_0$: The population means of the sepal length and the sepal width are not different across the species.

```{r}
# Summary MANOVA result with different test method
SepalSize <- cbind(iris$Sepal.Length, iris$Sepal.Width)
iris_manova <- manova(SepalSize ~ iris$Species)
summary(iris_manova, test = 'Pillai')  # default
summary(iris_manova, test = 'Wilks')
summary(iris_manova, test = 'Roy')
summary(iris_manova, test = 'Hotelling-Lawley')
# Univariate ANOVAs for each dependent variable
summary.aov(iris_manova)
```
**Conclusion:** The species has a statistically significant effect on the sepal width and sepal length.

## <span style="color:gray; font-family:Microsoft JhengHei;">**8.4**</span> **Post-hoc test**

**Example:** after One-way MANOVA gives a significant result, which group(s) is/are different from other(s)?  
**Hypothesis:** Linear Discriminant Analysis (LDA)
```{r, fig.align='center', message=FALSE}
# Visualization
library(MASS)
iris_lda <- lda(iris$Species ~ SepalSize, CV = FALSE)
plot_lda <- data.frame(Species = iris$Species, lda = predict(iris_lda)$x)
ggplot(plot_lda) + geom_point(aes(x = lda.LD1, y = lda.LD2, colour = Species))
```
**Conclusion:** The sepal size of the setosa species is different from other species.

## <span style="color:gray; font-family:Microsoft JhengHei;">**8.5**</span> **Multivariate normality**

### <span style="color:gray; font-family:Microsoft JhengHei;">**8.5.1**</span> **Shapiro-Wilk test**

**Hypothesis:**  
$H_0$: The variable follows a normal distribution
```{r, message=FALSE}
library(rstatix)
iris |>  
  group_by(Species) |>  
  shapiro_test(Sepal.Length, Sepal.Width)
```
Tip:

- <span style="color:green">If the sample size is large (say n > 50), the visual approaches such as QQ-plot and histogram will be better for assessing the normality assumption.</span>
```{r, fig.align='center', message=FALSE}
iris[, c('Species', 'Sepal.Length', 'Sepal.Width')] |> 
  pivot_longer(cols = c(Sepal.Length, Sepal.Width)) |> 
  ggplot() +
  geom_histogram(aes(value)) +
  facet_grid(name ~ Species)
```
**Conclusion:** As $p>0.05$, the sepal length and the width for each species are normally distributed.

### <span style="color:gray; font-family:Microsoft JhengHei;">**8.5.2**</span> **Mardiaâ€™s skewness and kurtosis test**

**Hypothesis:**  
$H_0$: The variables follow a multivariate normal distribution
```{r}
library(mvnormalTest)
mardia(iris[, c('Sepal.Length', 'Sepal.Width')])$mv.test
```
Tip:

- <span style="color:green">When n > 20 for each combination of the independent and dependent variable, the multivariate normality can be assumed (Multivariate Central Limit Theorem).</span>  

**Conclusion:** As $p>0.05$, the sepal length and the width follow a multivariate normal distribution.

### <span style="color:gray; font-family:Microsoft JhengHei;">**8.5.3**</span> **Homogeneity of the variance-covariance matrix**
**Main:**  
Boxâ€™s M test: Use a lower $\alpha$ level such as $\alpha = 0.001$ to assess the $p$ value for significance.  
**Hypothesis:**  
$H_0$: The variance-covariance matrices are equal for each combination formed by each group in the independent variable.
```{r, message=FALSE}
library(biotools)
boxM(cbind(iris$Sepal.Length, iris$Sepal.Width), iris$Species)
```
**Conclusion:** As $p < 0.001$, the variance-covariance matrices for the sepal length and width are not equal for each combination formed by each species.

### <span style="color:gray; font-family:Microsoft JhengHei;">**8.5.4**</span> **Multivariate outliers**
- [MANOVA is highly sensitive to outliers and may produce Type I or II errors.]{style="color:red"}
- Multivariate outliers can be detected using the Mahalanobis Distance test. The larger the Mahalanobis Distance, the more likely it is an outlier.
```{r}
library(rstatix)
iris_outlier <- mahalanobis_distance(iris[, c('Sepal.Length', 'Sepal.Width')])
head(iris_outlier, 5)
```

### <span style="color:gray; font-family:Microsoft JhengHei;">**8.5.5**</span> **Linearity**
- Or test the regression or the slope (ENV221)
```{r}
# Visualize the pairwise scatterplot for the dependent variable for each group
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  facet_wrap(Species ~ .)
```

### <span style="color:gray; font-family:Microsoft JhengHei;">**8.5.6**</span> **Multicollinearity**
Correlation between the dependent variable.  
For three or more dependent variables, use a correlation matrix or variance inflation factor (VIF).
```{r}
# Test the correlation
cor.test(x = iris$Sepal.Length, y = iris$Sepal.Width)
# Visualization
ggplot(iris, aes(Sepal.Length, Sepal.Width)) +
  geom_point() +
  geom_smooth(method = 'lm')
```
- If $|r|$ > 0.9, there is multicollinearity.
- If r is too low, perform separate univariate ANOVA for each dependent variable.

## <span style="color:gray; font-family:Microsoft JhengHei;">**8.6**</span> **Two-way MANOVA**

<center>![Two-way MANOVA](images/Two-way MANOVA.jpg)</center>
**Example:** Plastic. Do the rate of extrusion and the additive have influence on the plastic quality?
```{r}
# Summary MANOVA result
data('Plastic', package = 'heplots')
Plastic_matrix <- as.matrix(Plastic[, c('tear','gloss','opacity')])
Plastic_manova <- manova(Plastic_matrix ~ Plastic$rate * Plastic$additive)
summary(Plastic_manova)
# Univariate ANOVAs for each dependent variable
summary.aov(Plastic_manova)
```

# <span style="color:gray; font-family:Microsoft JhengHei;">**9**</span> **ANCOVA**

## <span style="color:gray; font-family:Microsoft JhengHei;">**9.1**</span> **Definition of ANCOVA**
<span style="color:red">Test whether the independent variable(s) has a significant influence on the dependent variable, excluding the influence of the covariate (preferably highly correlated with the dependent variable)</span>
$$Y_{ij} = (\mu+\tau_{i})+\beta(x_{ij}-\bar{x})+\epsilon_{ij}$$

- $Y_{ij}$: the j-th observation under the i-th categorical group
- $\mu$: the population mean
- $i$: groups, 1,2, â€¦
- $j$: observations, 1,2,â€¦
- $\tau_i$: an adjustment to the y intercept for the i-th regression line
- $\mu + \tau_i$: the intercept for group i
- $\beta$: the slope of the line
- $x_{ij}$: the j-th observation of the continuous variable under the i-th group
- $\bar x$: the global mean of the variable x
- $\epsilon _{ij}$: the associated unobserved error

**Analysis of covariance (ANCOVA):**

- Dependent variable (DV): One continuous variable
- Independent variables (IVs): One or multiple categorical variables, one or multiple continuous variables (covariate, CV)

**Covariate (CV):**

- An independent variable that is not manipulated by experimenters but still influences experimental results.

**Example:**

<center>![Model simplification](images/ANCOVA-plot.png)</center>

## <span style="color:gray; font-family:Microsoft JhengHei;">**9.2**</span> **One-way ANCOVA**

### <span style="color:gray; font-family:Microsoft JhengHei;">**9.2.1**</span> **Question**

<center>![ANCOVA](images/ANCOVA.jpg)</center>

**Example 1:** Does <u>grazing</u> have influence on the fruit production? Are grazed plants have more fruit <u>production</u> than ungrazed ones?

- Independent variable: 
   - Grazing (categorical)
- Dependent variable: 
   - Fruit production (continuous)
```{r, fig.width = 6, fig.height = 4, fig.align='center'}
df1 <- read.table("data/ipomopsis.txt", header = TRUE, stringsAsFactors = TRUE)
head(df1, 5)
tapply(df1$Fruit,df1$Grazing, mean)
library(ggplot2)
ggplot(df1) + geom_boxplot(aes(Fruit, Grazing))
# Hypothesis test
t.test(Fruit ~ Grazing, data = df1, alternative = c("greater"))
```
**Example 2:** What is the influence of <u>grazing</u> and <u>root diameter</u> on the fruit production of a plant?

Independent variables:

- grazing (categorical: grazed or ungrazed)
- root diameter (continuous, mm, covariate)

Dependent variable:

- fruit production (mg)

```{r}
# Visualization
ggplot(df1, aes(Root, Fruit))+
  geom_point() +
  geom_smooth(method = 'lm') +
  geom_point(aes(color = Grazing)) + 
  geom_smooth(aes(color = Grazing), method = 'lm')
```

### <span style="color:gray; font-family:Microsoft JhengHei;">**9.2.2**</span> **Maximal model**

| Symbol | Meaning |
|:---:|:-----|
| `~` | Separating DV (<span style="color:red">left</span>) and IV (<span style="color:red">right</span>) |
| `:` | Interaction effect of two factors |
| `*` | Main effect of the two factors and the interaction effect. `f1 * f2` is equivalent to `f1 + f2 + f1:f2` |
| `^` | Square the sum of several terms. The main effect of these terms and the interaction between them |
| `.` | All variables except the DV |
```{r}
# The maximal model
df1_ancova <- lm(Fruit ~ Grazing * Root, data = df1)
summary(df1_ancova)
# The ANOVA table for the maximal model
anova(df1_ancova)
# other method to see the ANOVA table 
df1_aov <- aov(Fruit ~ Grazing * Root, data = df1)
summary(df1_aov)
summary.aov(df1_ancova)
```

### <span style="color:gray; font-family:Microsoft JhengHei;">**9.2.3**</span> **Minimal model**

```{r}
# Delete the interaction factor 
df1_ancova2 <- update(df1_ancova, ~ . - Grazing:Root)
summary(df1_ancova2)
# Compare the simplified model with the maximal model
anova(df1_ancova, df1_ancova2)
# Delete the grazing factor 
df1_ancova3 <- update(df1_ancova2, ~ . - Grazing)
summary(df1_ancova3)
# Compare the two models
anova(df1_ancova2, df1_ancova3)
summary(df1_ancova2)
anova(df1_ancova2)
```

### <span style="color:gray; font-family:Microsoft JhengHei;">**9.2.4**</span> **One step**

Criterion: Akaikeâ€™s information criterion (AIC). The model is <u>worse</u> if AIC gets <u>greater</u>.
```{r}
step(df1_ancova)
```


### <span style="color:gray; font-family:Microsoft JhengHei;">**9.2.5**</span> **Result**

```{r}
# Extracting formulas from linear regression models
equatiomatic::extract_eq(df1_ancova2, use_coefs = TRUE)
```
<a id="diagnostic statistical data text table"></a>
```{r}
# Create a diagnostic statistical data text table
stargazer::stargazer(df1_ancova2, type = 'text')
```
<details>
  <summary>The meaning of each parameter in this table</summary>
- Dependent variable: The name of the dependent variable. (å› å˜é‡çš„åç§°)
- Independent variables: The names of the independent variables. (è‡ªå˜é‡çš„åç§°)
- Coefficients: Regression coefficients, indicating the degree to which an independent variable affects the dependent variable when it increases by one unit. (å›å½’ç³»æ•°ï¼Œè¡¨ç¤ºè‡ªå˜é‡æ¯å¢åŠ ä¸€ä¸ªå•ä½å¯¹å› å˜é‡çš„å½±å“ç¨‹åº¦)
- Standard errors: Standard error of regression coefficients, measuring the stability of regression coefficients. (å›å½’ç³»æ•°çš„æ ‡å‡†è¯¯å·®ï¼Œè¡¡é‡å›å½’ç³»æ•°çš„ç¨³å®šæ€§)
- t-statistics: t-value of regression coefficients, representing whether a regression coefficient is significantly different from zero and has a significant impact on the dependent variable. (å›å½’ç³»æ•°çš„tå€¼ï¼Œä»£è¡¨å›å½’ç³»æ•°æ˜¯å¦æ˜¾è‘—ä¸ä¸º0ï¼Œå³æ˜¯å¦å¯¹å› å˜é‡æœ‰æ˜¾è‘—å½±å“)
- p-values: Significance level of regression coefficients, usually used to determine whether a regression coefficient is significantly different from zero. The smaller the p-value, the more significant the regression coefficient is considered to be. (å›å½’ç³»æ•°çš„æ˜¾è‘—æ€§æ°´å¹³ï¼Œé€šå¸¸ç”¨äºåˆ¤æ–­å›å½’ç³»æ•°æ˜¯å¦æ˜¾è‘—ä¸ä¸º0ï¼Œpå€¼è¶Šå°ï¼Œè¡¨ç¤ºå›å½’ç³»æ•°è¶Šæ˜¾è‘—)
- Observations: Sample size. (æ ·æœ¬æ•°é‡)
- R2: Goodness-of-fit measure that represents how much variance in explanatory variables can be explained by model. A higher value indicates better fit between model and data. (æ‹Ÿåˆä¼˜åº¦ï¼Œè¡¨ç¤ºæ¨¡å‹è§£é‡Šå˜é‡æ–¹å·®çš„æ¯”ä¾‹ï¼Œæ•°å€¼è¶Šé«˜è¡¨ç¤ºæ¨¡å‹æ‹Ÿåˆç¨‹åº¦è¶Šå¥½)
- Adjusted R2ï¼šA modified version of R2 that takes into account number of independent variables for greater accuracy. (è°ƒæ•´åçš„æ‹Ÿåˆä¼˜åº¦ï¼Œè€ƒè™‘åˆ°è‡ªå˜é‡çš„ä¸ªæ•°ï¼Œæ¯”R2æ›´å‡†ç¡®)
- Residual standard errorï¼šStandard deviation or dispersion measure for residuals; smaller values indicate better model fit. (æ®‹å·®æ ‡å‡†è¯¯ï¼Œè¡¨ç¤ºæ®‹å·®çš„ç¦»æ•£ç¨‹åº¦ï¼Œè¶Šå°è¡¨ç¤ºæ¨¡å‹è¶Šå¥½)
- F-statisticï¼šStatistical test used to evaluate overall goodness-of-fit for linear models. (Fç»Ÿè®¡é‡ï¼Œç”¨äºæ£€éªŒæ¨¡å‹æ•´ä½“æ‹Ÿåˆä¼˜åº¦æ˜¯å¦æ˜¾è‘—)
- dfï¼šDegrees of freedom. (è‡ªç”±åº¦)
- Note:p<0.1,p<0.05,p<0.01 : When p-value is less than 0.1, 0.05 or 0.01 respectively,* , ** , *** are used as symbols indicating significance levels for corresponding regressions coefficients. (p<0.1, p<0.05, p<0.01ï¼špå€¼å°äº0.1ï¼Œ0.05ï¼Œ0.01æ—¶ï¼Œåˆ†åˆ«ç”¨*ï¼Œ**ï¼Œ***è¡¨ç¤ºï¼Œä»£è¡¨å›å½’ç³»æ•°çš„æ˜¾è‘—æ€§æ°´å¹³)
</details>

## <span style="color:gray; font-family:Microsoft JhengHei;">**9.3**</span> **Two-way ANCOVA**

<center>![ANCOVA](images/ANCOVA.jpg)</center>

### <span style="color:gray; font-family:Microsoft JhengHei;">**9.3.1**</span> **Question**

Previous experiments have shown that both genotype and sex of an organism affect body weight gain. However, a scientist believes that after adjusting for <u>age</u>, there was no significant difference in means of <u>weight</u> gain between groups at different levels of <u>sex</u> and <u>Genotype</u>. Can experiments support this claim?

- Independent variables: 
   - genotype (categorical)
   - sex (categorical)
   - age (covariate)
- Dependent variable: 
   - Weight gain (continuous)

### <span style="color:gray; font-family:Microsoft JhengHei;">**9.3.2**</span> **Model**

```{r}
Gain <- read.table("data/Gain.txt", header = T)
head(Gain, 3)
m1 <- lm(Weight ~ Sex * Age * Genotype, data = Gain)
summary(m1)
```
- There are no things like Age:Sex or Age:Genotype, so the slope of weight gain against age does not vary with sex or genotype
- In the final minimal adequate model, three main effects were included (Sex, Age, Genotype), so it can be considered that there are intercept differences between gender, age and genotype (intercepts vary).
- The final minimal adequate model includes three main effects (Sex, Age, Genotype), but no interaction effect. This means that the effects of these variables are independent and there is no interaction between them.

### <span style="color:gray; font-family:Microsoft JhengHei;">**9.3.3**</span> **One step**

```{r}
m2 <- step(m1)
summary(m2)
```
From the above output, we can see the coefficients of each genotype and their corresponding significance level (in the Estimate column). It can be observed that GenotypeCloneC and GenotypeCloneE have similar effects on the dependent variable, with coefficients close to -1 and very small significance levels (p-value < 0.001). Therefore, we can combine these two factors into one factor, reducing the number of genotype levels from six to five. The same approach can be applied to B and D.
```{r}
# Check
newGenotype <- as.factor(Gain$Genotype)
levels(newGenotype)
# Change
levels(newGenotype)[c(3,5)] <- "ClonesCandE"
levels(newGenotype)[c(2,4)] <- "ClonesBandD"
levels(newGenotype)
# Liner regression & compare
m3 <- lm(Weight ~ Sex + Age + newGenotype, data = Gain)
anova(m2,m3)
```
<details>
  <summary>Analysis of RSS above</summary>
In regression models, the residual sum of squares (RSS) represents the unexplained variance in the dependent variable. In this example, the RSS for Model 1 and Model 2 are 2.7489 and 2.9938 respectively. The goal of a model is to minimize RSS because it represents how well the model fits with observed values. Generally, a smaller RSS indicates a better fit for the model. When comparing two models' fit, one can use both RSS and F-statistic. In this example, although Model 2 has a slightly larger RSS than Model 1, its P-value for F-statistic is 0.1087 which does not reach commonly used significance levels such as 0.05; therefore we cannot reject null hypothesis that Model 2 performs worse than Model 1.
</details>

### <span style="color:gray; font-family:Microsoft JhengHei;">**9.3.4**</span> **Result**

As $p=0.1087$, there is no significant difference between the two models. Therefore, the new model uses NewGenotype (four levels) instead of Genotype (six levels).
```{r}
summary(m3)
# Extracting formulas from linear regression models
equatiomatic::extract_eq(m3, use_coefs = TRUE)
# Create a diagnostic statistical data text table
stargazer::stargazer(m3, type = 'text')
```
[The meaning of each parameter in this table](#diagnostic statistical data text table)

### <span style="color:gray; font-family:Microsoft JhengHei;">**9.3.5**</span> **Visualization**

```{r}
plot(Weight~Age,data=Gain,type="n")
colours <- c("green","red","black","blue")
lines <- c(1,2)
symbols <- c(16,17)
NewSex<-as.factor(Gain$Sex)
points(Weight~Age,data=Gain,pch=symbols[as.numeric(NewSex)],
       col=colours[as.numeric(newGenotype)])
xv <- c(1,5)
for (i in 1:2) {
  for (j in 1:4){
    a <- coef(m3)[1]+(i>1)* coef(m3)[2]+(j>1)*coef(m3)[j+2]
 
    b <- coef(m3)[3]
    yv <- a+b*xv
    lines(xv,yv,lty=lines[i],col=colours[j]) } }
```

# <span style="color:gray; font-family:Microsoft JhengHei;">**10**</span> **MANCOVA**

<center>![One-Way MANCOVA](images/One-Way MANCOVA.jpg)</center>
<center>![Two-Way MANCOVA](images/Two-Way MANCOVA.jpg)</center>
- Dependent variables: multiple continuous variables
- Independent variables: one or multiple categorical variables, one or multiple continuous variables (covariates)

## <span style="color:gray; font-family:Microsoft JhengHei;">**10.1**</span> **Definition of MANCOVA**

<span style="color:red">**Multivariate Analysis of Covariance (MANCOVA) = multivariate ANCOVA = MANOVA with covariate(s)**</span><br>
Analysis for the differences among group means for a linear combination of the dependent variables after adjusted for the covariate. Test whether the independent variable(s) has a significant influence on the dependent variables, excluding the influence of the covariate (preferably highly correlated with the dependent variable)

- Independent Random Sampling: Independence of observations from all other observations.
- Level and Measurement of the Variables: The independent variables are categorical and the dependent variables are continuous or scale variables. Covariates are continuous.
- Homogeneity of Variance: Variance between groups is equal.
- Normality: For each group, each dependent variable follows a normal distribution and any linear combination of dependent variables are normally distributed

<details>
  <summary>Brief explanation in Chinese</summary>
åœ¨MANCOVAä¸­ï¼Œæˆ‘ä»¬æœ‰å¤šä¸ªå› å˜é‡ï¼ˆå³è¿ç»­æˆ–æ¯”ä¾‹å˜é‡ï¼‰ï¼Œä¸€ä¸ªæˆ–å¤šä¸ªè‡ªå˜é‡ï¼ˆå³åˆ†ç±»å˜é‡ï¼‰ï¼Œä»¥åŠä¸€ä¸ªæˆ–å¤šä¸ªåå˜é‡ï¼ˆå³è¿ç»­å˜é‡ï¼‰ã€‚è¿™äº›å˜é‡çš„æµ‹é‡æ°´å¹³åº”è¯¥æ­£ç¡®ï¼Œå¹¶ä¸”è§‚å¯Ÿå€¼åº”è¯¥æ˜¯ç‹¬ç«‹éšæœºé‡‡æ ·çš„ã€‚è¿™æ„å‘³ç€ï¼Œæˆ‘ä»¬è¦ç¡®ä¿è§‚å¯Ÿå€¼å½¼æ­¤ç‹¬ç«‹ï¼Œä¸å—å…¶ä»–å˜é‡çš„å½±å“ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ£€æŸ¥æ¯ä¸ªç»„çš„å› å˜é‡æ˜¯å¦éƒ½ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼Œå¹¶ä¸”æ–¹å·®åº”è¯¥ç›¸ç­‰ã€‚  
å¦‚æœæˆ‘ä»¬çš„æ•°æ®æ»¡è¶³è¿™äº›å‡è®¾ï¼Œåˆ™å¯ä»¥ä½¿ç”¨MANCOVAæ¥æ¢ç´¢è‡ªå˜é‡å¯¹å› å˜é‡çš„å½±å“ï¼Œå¹¶ä¸”é€šè¿‡åå˜é‡æ¥æ§åˆ¶ä¸€äº›å…¶ä»–å½±å“å› ç´ çš„å½±å“ã€‚MANCOVAå¯ä»¥æ›´å‡†ç¡®åœ°ç¡®å®šç»„é—´å·®å¼‚æ˜¯å¦çœŸå®å­˜åœ¨ï¼Œè€Œä¸æ˜¯åŸºäºå•ä¸ªå› å˜é‡çš„åˆ†ææ¥åšå‡ºåˆ¤æ–­ã€‚ MANCOVAé€šå¸¸ç”¨äºå®éªŒè®¾è®¡ï¼Œç ”ç©¶äººå‘˜æƒ³è¦æ¯”è¾ƒå¤šä¸ªç»„çš„å¹³å‡å¾—åˆ†ï¼ŒåŒæ—¶æ§åˆ¶å…¶ä»–å› ç´ çš„å½±å“ã€‚
</details>

## <span style="color:gray; font-family:Microsoft JhengHei;">**10.2**</span> **Workflow**

### <span style="color:gray; font-family:Microsoft JhengHei;">**10.2.1**</span> **Question**

**Example:** Are there differences in productivity (measured by income and hours worked) for individuals in different age groups after adjusted for the education level?  

- Dependent variables:
   - wage (continuous)
   - age (continuous)
- Independent variables:
   - education (categorical)
   - year (continuous, covariate)

### <span style="color:gray; font-family:Microsoft JhengHei;">**10.2.2**</span> **Visualization**

```{r, message=FALSE}
library(tidyverse)
library(ISLR)
library(car)
ggplot(Wage, aes(age, wage)) + geom_point(alpha = 0.3) + 
  geom_smooth(method = lm) + facet_grid(year~education)
```

### <span style="color:gray; font-family:Microsoft JhengHei;">**10.2.3**</span> **Model**

- `manova()` way
```{r}
wage_manova1 <- manova(cbind(wage, age) ~ education * year, data = Wage)
wage_manova1
summary.aov(wage_manova1)
```
<details>
  <summary>Brief explanation in Chinese</summary>
æ ¹æ®ç»“æœï¼Œæ•™è‚²å’Œå¹´ä»½ä¸¤ä¸ªå› å­å¯¹ wage æœ‰æ˜¾è‘—å½±å“ï¼Œå› ä¸ºå®ƒä»¬çš„ P å€¼å‡å°äº0.05ã€‚ä½†æ˜¯ï¼Œäº¤äº’ä½œç”¨é¡¹ "education:year" çš„ P å€¼å¤§äº0.05ï¼Œè¡¨æ˜è¿™ä¸ªäº¤äº’é¡¹å¯¹ wage æ²¡æœ‰æ˜¾è‘—å½±å“ã€‚<br><br> 
å¯¹äºå¹´é¾„å˜é‡ï¼Œæ•™è‚²å’Œ(å¹´ä»½â‰ˆ0.05)éƒ½å¯¹å…¶æœ‰æ˜¾è‘—å½±å“ï¼Œå› ä¸ºå®ƒä»¬çš„ P å€¼å°äº0.05ã€‚ç„¶è€Œï¼Œäº¤äº’é¡¹ "education:year" å¯¹ age æ²¡æœ‰æ˜¾è‘—å½±å“ï¼Œå› ä¸ºå…¶ P å€¼å¤§äº0.05ã€‚
</details>

- `jmv::mancova()` way
```{r}
library(jmv)
wage_manova2 <- jmv::mancova(data = Wage,
                             deps = vars(wage, age),
                             factors = education, 
                             covs = year)
wage_manova2
```
<details>
  <summary>Brief explanation in Chinese</summary>
åœ¨ Multivariate Tests è¡¨æ ¼ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° education å’Œ year è¿™ä¸¤ä¸ªè‡ªå˜é‡çš„å¤šå…ƒå‡è®¾æ£€éªŒç»“æœã€‚å››ç§ç»Ÿè®¡é‡éƒ½æ˜¾ç¤ºäº†è¿™ä¸¤ä¸ªå˜é‡çš„ç»„åˆå¯¹å› å˜é‡ wage å’Œ age æœ‰æ˜¾è‘—å½±å“ï¼ˆp å€¼ < 0.0001ï¼‰ã€‚<br><br>
åœ¨ Univariate Tests è¡¨æ ¼ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯ä¸ªå› å˜é‡ wage å’Œ age çš„å•å…ƒå‡è®¾æ£€éªŒç»“æœã€‚ç»“æœæ˜¾ç¤º education å¯¹ wage å’Œ age éƒ½æœ‰æ˜¾è‘—å½±å“ï¼ˆp å€¼ < 0.0001ï¼‰ï¼Œè€Œ year åªå¯¹ wage æœ‰æ˜¾è‘—å½±å“ï¼ˆp å€¼ = 0.0004ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œage çš„ year æ•ˆåº”åœ¨ p å€¼ä¸º 0.0529 çš„è¾¹ç¼˜ã€‚Residuals åˆ—æ˜¾ç¤ºäº†æ¯ä¸ªå› å˜é‡åœ¨æ¨¡å‹ä¸­çš„æ®‹å·®æ–¹å·®ã€‚<br><br>
æ€»ä½“æ¥è¯´ï¼Œè¿™ä¸ª MANCOVA æ¨¡å‹è¡¨æ˜ï¼Œåœ¨æ§åˆ¶äº† education å’Œ year ä¹‹åï¼Œwage å’Œ age ä¹‹é—´å­˜åœ¨ç€ç›¸å…³æ€§ï¼Œå¹¶ä¸” education å’Œ year å¯¹è¿™ç§å…³ç³»éƒ½æœ‰æ˜¾è‘—çš„å½±å“ã€‚
</details>

### <span style="color:gray; font-family:Microsoft JhengHei;">**10.2.4**</span> **Result**

- Since the interaction effect is not significant (p = 0.77 for salary and p = 0.24 for age), the slopes are parallel.
- The wage and age differ significantly among education groups (p for both wage and age are far below 0.05).
- Differences in salary also significantly (p = 0.00038) increase over time (variable year), due to some economic reasons, while differences in age donâ€™t change (p = 0.053) much.

<details>
  <summary>Brief explanation in Chinese</summary>
- å¯¹äºäº¤äº’ä½œç”¨æ•ˆåº”è€Œè¨€ï¼Œå®ƒåœ¨wageå’Œageæ–¹é¢çš„på€¼åˆ†åˆ«ä¸º0.771086å’Œ0.24043ï¼Œå› æ­¤äº¤äº’ä½œç”¨æ•ˆåº”ä¸æ˜¾è‘—ã€‚è¿™æ„å‘³ç€ä¸åŒæ•™è‚²æ°´å¹³ç»„ä¹‹é—´çš„å·¥èµ„å’Œå¹´é¾„å·®å¼‚çš„æ–œç‡æ˜¯å¹³è¡Œçš„ã€‚
- å¯¹äºMANOVAè¡¨æ ¼ä¸­çš„â€œMultivariate Testsâ€éƒ¨åˆ†ï¼Œeducationç»„ä¹‹é—´çš„å·®å¼‚åœ¨wageå’Œageæ–¹é¢éƒ½æ˜¾è‘—(på€¼å‡è¿œè¿œä½äº0.05)ã€‚
- å¯¹äºANOVAè¡¨æ ¼ä¸­çš„â€œUnivariate Testsâ€éƒ¨åˆ†ï¼Œéšæ—¶é—´çš„æ¨ç§»ï¼Œsalaryæ–¹é¢çš„å·®å¼‚æ˜¾è‘—å¢åŠ (p=0.000384)ï¼Œè€Œageæ–¹é¢çš„å·®å¼‚åˆ™æ²¡æœ‰æ˜¾è‘—å˜åŒ–(p=0.05282)ã€‚è¿™è¡¨æ˜æ—¶é—´å˜åŒ–æ˜¯é€ æˆsalaryå˜åŒ–çš„ä¸€ä¸ªé‡è¦åŸå› ï¼Œè€Œä¸æ˜¯å¹´é¾„å˜åŒ–ã€‚
</details>

# <span style="color:gray; font-family:Microsoft JhengHei;">**11**</span> **Combining statistics**

## <span style="color:gray; font-family:Microsoft JhengHei;">**11.1**</span> **Combining means and standard errors**

**Example:** Two separate but similar experiments measuring the rate of glucose production of liver cells.<br>
*Tips: for different group experments we can't just sum them up and simply calculate the mean and standard error.*

- Experiment 1: 4.802, 3.81, 4.004, 4.467, 3.8
- Experiment 2: 5.404, 5.256, 4.145, 5.401, 5.622, 4.312
- Calculate the overall $n, \bar x, se$

**If we know the row data:**
```{r}
# Row data
x1 <- c(4.802, 3.81, 4.004, 4.467, 3.8)
x2 <- c(5.404, 5.256, 4.145, 5.401, 5.622, 4.312)
# Form a new dataframe which contains Those columns
n1 <- length(x1)
n2 <- length(x2)
dtf <- data.frame(n = c(n1, n2), mean = c(mean(x1), mean(x2)), sd = c(sd(x1), sd(x2)))
dtf$se <- dtf$sd/sqrt(dtf$n)
dtf
# Calculate the mean and standard error
x <- c(x1, x2)
x_bar <- mean(x)
n <- length(x)
se <- sd(x)/sqrt(n)
c(x_bar, se)
```

**If we don't know the row data:**
```{r}
cmean <-  sum(dtf$mean * dtf$n) / sum(dtf$n)
cse <- sqrt((sum(dtf$n * ((dtf$n - 1)* dtf$se ^ 2 + dtf$mean ^ 2)) - sum(dtf$n * dtf$mean) ^ 2/ sum(dtf$n)) / (sum(dtf$n) * (sum(dtf$n) - 1)))
c(cmean, cse)
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**11.2**</span> **Mean and standard errors of sum and difference**

### <span style="color:gray; font-family:Microsoft JhengHei;">**11.2.1**</span> **Concepts**

- First we have two variables $p$ and $q$
- The third variable $x = p + q$
- The fourth variable $y = p - q$
- When $n_p \neq n_q \neq n$:
$$\bar x = \bar p + \bar q$$
$$\bar y = \bar p - \bar q$$
$$se_x = se_y = \sqrt{\frac{(n_p - 1)se_p^2 + (n_q - 1)se_q^2}{n_p + n_q - 2} \cdot \frac{n_p + n_q}{n_p n_q}}$$
- When $n_p = n_q = n$:
$$se_x = se_y = \sqrt{\frac{se_p^2 + se_q^2}{n}}$$

### <span style="color:gray; font-family:Microsoft JhengHei;">**11.2.2**</span> **Example**

A luciferase-based assay is being used to quantify the amount of ATP and ADP in small tissue samples. The amount of ATP (q) is measured directly in 8 samples as $3.25 \pm 0.14 mol g^{-1}$. A further 10 samples are treated with pyruvate kinase plus phosphoenolpyruvate to convert ADP quantitatively to ATP. The total ATP (p) in these samples is determined to be $4.56 \pm 0.29\mu mol g^{-1}$. The ADP content is $p - q$.<br><br>
What is the mean and standard error of ADP concentration?
```{r}
# Dataframe of example experiment
x <- data.frame(mean = c(3.25, 4.56), n = c(8, 10), se = c(0.14, 0.29))
x
ADP <- diff(x$mean)
cse <- sqrt(sum(x$se ^ 2 * (x$n - 1)) / (sum(x$n) - 2) * sum(x$n) / prod(x$n))
c(ADP, cse)
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**11.3**</span> **Mean and standard error of ratios and products**

### <span style="color:gray; font-family:Microsoft JhengHei;">**11.3.1**</span> **Concepts**

- First we have two variables $p$ and $q$
- The third variable $x = p \cdot q$
- The fourth variable $y = p / q$
$$\bar x = \bar p \cdot \bar q$$
$$\bar y = \bar p / \bar q$$
$$se_x = \sqrt{\frac{\bar p^2 n_q se_q^2 + \bar q^2 n_p se_p^2 + n_p se_p^2 n_q se_q^2}{n_p + n_q - 2}}$$
$$se_y = \frac{1}{\bar q} \sqrt {\frac{n_p se_p^2 + n_qse_q^2(\frac{\bar p}{\bar q})^2}{n_p + n_q - 2}}$$

### <span style="color:gray; font-family:Microsoft JhengHei;">**11.3.2**</span> **Example**

In the previous example, we got the concentrations of ATP and ADP in a tissue sample. what is the ratio of [ATP]/[ADP]?
```{r}
x <- data.frame(mean = c(3.25, ADP), n = c(8, 10), se = c(0.14, cse))
x
ratiox <- x$mean[1] / x$mean[2]
cse <- sqrt((x$n[1] * x$se[1] ^ 2 + x$n[2] * x$se[2] ^ 2 * ((x$mean[1] / x$mean[2]) ^ 2)) / (sum(x$n) - 2)) / x$mean[2]
c(ratiox, cse)
```

# <span style="color:gray; font-family:Microsoft JhengHei;">**12**</span> **Non-parametric hypothesis tests**

## <span style="color:gray; font-family:Microsoft JhengHei;">**12.1**</span> **Definition**

**Non-parametric hypothesis tests:**<br>
A hypothesis test that does not require any specific conditions concerning the shapes of <mark>population distributions</mark> or <mark>the values of population parameters</mark>. When we don't know the distribution of the population, the it is <mark>easier to perform</mark> than corresponding parametric tests but <mark>less efficient</mark> than parametric tests.

## <span style="color:gray; font-family:Microsoft JhengHei;">**12.2**</span> **Wilcoxon Rank-Sum test**

### <span style="color:gray; font-family:Microsoft JhengHei;">**12.2.1**</span> **Concept**

Wilcoxon Rank-Sum test, also known as the Mann-Whitney U test, is a non-parametric test for <mark>comparing the equality of the medians of two independent samples</mark>. Its original hypothesis is that the medians of the two samples are equal, and the alternative hypothesis is that the medians of the two samples are not equal.<br>

- $H_0$: the medians of the two populations are equal.
  - Both samples are drawn randomly and independently.
  - The measures within the two samples are able to be ranked and hence must be continuous, discrete or ordinal.

The basic idea of this test is to combine the two samples, rank them in order of size, and then calculate how many of the rankings in each sample are less than or equal to the value of that sample. The sum of these rankings is then used as the test statistic. If the medians of the two samples are equal, the test statistic should be close to half the sum of the total rankings. If the medians of the two samples are not equal, then the test statistic should be far from half the sum of the total rankings.
$$U=\sum_{i=1}^n \sum_{j=1}^m S\left(X_i, Y_j\right)$$
$$S(X, Y)= \begin{cases}1, & \text { if } X>Y \\ \frac{1}{2}, & \text { if } X=Y \\ 0, & \text { if } X<Y\end{cases}$$

- $U$: The Wilcoxon Rank-Sum test statistic
- $X_1, \ldots, X_n$: an independent sample from $X$
- $Y_1, \ldots, Y_m$: an independent sample from $Y$

### <span style="color:gray; font-family:Microsoft JhengHei;">**12.2.2**</span> **Example-1**

**Suppose we have two dataset x and y, calculate the U (U is the smaller rank sum of these two samples):**
```{r}
# Row data
x <- c(3, 7)
y <- c(2, 4, 5, 8, 9)
m <- length(x)
n <- length(y)
# One step
# The outer() function is the function used in R to perform various operations between two vectors, 
# returning the result of all operations between all elements of the two vectors.
U <- sum(outer(x, y, ">")) + sum(outer(x, y, "==")) * 0.5 +  sum(outer(x, y, "<")) * 0
U
```

**Here is the distribution of Wilcoxon Rank-Sum test statistic:**<br>
The larger of the m and n, the U more like normal distribution, approximately normal distribution by:<br>
$$\mu_U = mn/2$$
$$s_U = \sqrt{\frac{mn(m + n + 1)}{12}}$$
<center>![Wilcoxon Rank-Sum test](images/Wilcoxon Rank-Sum test.png)</center>

If there have ties, $s_U$ need to be corrected as:
$$s_U = \sqrt{\frac{mn(m + n + 1)}{12}  - \frac{mn\sum((T_i - 1) T_i(T_i+1))}{12(m + n)((m+n)^2-1)}}$$
<span style="color:gray"><center>*$T_i$ is the number of ties in the ith set of ties*</center></span><br>

Ties concept:
```{r}
# Duplicate value ranking
rank(c(1, 2, 2, 3))
```

### <span style="color:gray; font-family:Microsoft JhengHei;">**12.2.3**</span> **Workflow**

```{r, fig.width = 6, fig.height = 4, fig.align='center'}
# One step
wilcox.test(x, y)
dtf <- data.frame(value = c(x, y),
                  name = rep(c('x', 'y'), c(m, n)))
wilcox.test(value ~ name, data = dtf)
# Boxplot visualization
boxplot(x, y, horizontal = TRUE)
U_critical <- qwilcox(c(0.025, 0.975), m, n)
pwilcox(U, m, n) * 2
# Probability density function plot visualization
curve(dwilcox(x, m, n), from = -1, to = 15, n = 17, ylab = 'Probability distribution density', las = 1, type = 'S', ylim = c(0, 0.15))
abline(h = 0)
segments(x0 = c(U, U_critical), y0 = 0, x1 = c(U, U_critical), y1 = dwilcox(c(U, U_critical), m, n), col = c('blue', 'red', 'red'))
legend('topright', legend = c('Distribution curve', 'Critical values', 'U score'), col = c('black', 'red', 'blue'), lty = 1, cex = 0.7)
```

### <span style="color:gray; font-family:Microsoft JhengHei;">**12.2.4**</span> **Example-2**

The insect spray dataset for C, D and F (just the pick C and D). The number of insects that survived when treated with insecticide treatments.
```{r, fig.width = 6, fig.height = 4, fig.align='center', warning=FALSE}
# Row data
insect <- read.csv("data/InsectSprays.csv")
insect <- insect[insect$spray %in% c("C", "D"), ]
# Visualization
boxplot(bugs ~ spray, data = insect, horizontal = TRUE)
```
Use Shapiro test to test each of the group we treat to see if they are normal distribution:
```{r}
shapiro.test(insect$bugs[insect$spray == "C"])
shapiro.test(insect$bugs[insect$spray == "D"])
```
Instead, we could test whether the two medians are different:
```{r}
tapply(insect$bugs, insect$spray, median)
```

**$H_0$:** The median of the survived insects treated by C is equal to that by D.
```{r, warning=FALSE}
# Step by step
m <- sum(insect$spray == 'C')
n <- sum(insect$spray == 'D')
x <- insect$bugs[insect$spray == 'C']
y <- insect$bugs[insect$spray == 'D']
U1 <- sum(outer(x, y, ">")) + sum(outer(x, y, "==")) * 0.5
U2 <- sum(outer(y, x, ">")) + sum(outer(y, x, "==")) * 0.5
U <- min(c(U1, U2))
U_critical <- qwilcox(c(0.025, 0.975), m, n)
pwilcox(U, m, n) * 2
# Approximately z distribution
U_mu <- m * n / 2
U_sd <- sqrt(m * n * (m + n + 1) / 12)
z_score <- (U - U_mu)/U_sd
pnorm(z_score) * 2 
# Determine if there is a significant difference between those two sprays
median(outer(insect$bugs[insect$spray == 'C'], insect$bugs[insect$spray == 'D'], "-"))
# One step
wilcox.test(bugs ~ spray, data = insect, conf.int=TRUE)
```

<details>
  <summary>The meaning of each parameter in this table</summary>
- W: the Wilcoxon test statistic U.
- We are 95% certain that the median difference between spray D and C across the population will be between 1 and 4 bugs.
- It does not estimate the difference in medians but rather the median of the difference (between the two samples)
</details>

**Decision:** Reject $H_0$<br>
**Conclusion:** There is a significant difference in insecticide effectiveness.

## <span style="color:gray; font-family:Microsoft JhengHei;">**12.3**</span> **Wilcoxon Signed-Rank test**

### <span style="color:gray; font-family:Microsoft JhengHei;">**12.3.1**</span> **Concept**

Test whether the median of the observed differences deviates enough from zero, based on paired samples. Roughly a <mark>non-parametric version of paired t-test</mark>.<br>
**$H_0$:** the medians of the two populations are equal.

### <span style="color:gray; font-family:Microsoft JhengHei;">**12.3.2**</span> **Example**

Ten people take part in a weight-loss program. They weigh before starting the program, and weigh again after the one-month program. Does the program have effect on the weight?

```{r, warning=FALSE}
# Row data
wl <- data.frame(
  id = LETTERS[1:10],
  before = c(198, 201, 210, 185, 204, 156, 167, 197, 220, 186),
  after = c(194, 203, 200, 183, 200, 153, 166, 197, 215, 184))
# Step by step
n <- nrow(wl)
d <- wl$after - wl$before
R <- rank(abs(d)) * abs(d) / d # signed ranks of the differences
W <- sum(R, na.rm = TRUE)
sW <- sqrt(n * (n + 1) * (2 * n + 1) / 6) # population standard deviation of W
z <- (abs(W) - 0.5) / sW
z_critical <- qnorm(p = c(0.025, 0.975)) # alpha = 0.05
pnorm(z, lower.tail = FALSE) * 2
# One step
wilcox.test(wl$before, wl$after, paired = TRUE, conf.int = TRUE, correct = FALSE)
```

**Decision:** As $p < 0.05$, reject $H_0$.<br>
**Conclusion:** The two population medians are different at $\alpha = 0.05$. The program has effect on the weight.

## <span style="color:gray; font-family:Microsoft JhengHei;">**12.4**</span> **Kruskal-Wallis test**

### <span style="color:gray; font-family:Microsoft JhengHei;">**12.4.1**</span> **Concept**

Test whether three or more population medians are equal. Roughly a <mark>non-parametric version of ANOVA</mark><br>
**$H_0$:** the medians of the populations are equal.

### <span style="color:gray; font-family:Microsoft JhengHei;">**12.4.2**</span> **Example**

The insect spray dataset for C, D, and F. The number of insects that survived when treated with insecticide treatments.<br>
Do any difference in the number of insects that survived when treated with multiple available insecticide treatments?
```{r, fig.width = 6, fig.height = 4, fig.align='center', warning=FALSE}
# Row data
insect <- read.csv("data/InsectSprays.csv")
# Visualization
boxplot(bugs ~ spray, data = insect, horizontal = TRUE, notch = TRUE)
shapiro.test(insect$bugs[insect$spray == "C"])  # Normal distribution
shapiro.test(insect$bugs[insect$spray == "D"])  # Normal distribution
shapiro.test(insect$bugs[insect$spray == "F"])  # Not normal distribution
# Determine if there is a significant difference between those three sprays
tapply(insect$bugs, insect$spray, median)
# One step
kruskal.test(bugs ~ spray, data = insect)
```
**Decision:** As $p < 0.05$, reject H0.<br>
**Conclusion:** There is a significant difference in insecticide effectiveness.

## <span style="color:gray; font-family:Microsoft JhengHei;">**12.5**</span> **Friedmanâ€™s test**

### <span style="color:gray; font-family:Microsoft JhengHei;">**12.5.1**</span> **Concept**

Test whether three or more population medians are equal for repeated measures. Roughly a <mark>non-parametric version of repeated measures ANOVA</mark>. <u>More powerful than ANOVA for very skewed (heavy tail) distributions</u>.
$$F_{r}=\frac{12}{n k(k+1)}\left(T_{1}^{2}+T_{2}^{2}+\ldots+T_{k}^{2}\right)-3 n(k+1)$$

### <span style="color:gray; font-family:Microsoft JhengHei;">**12.5.2**</span> **Example**

```{r}
# Row data
dtf <- data.frame(
  before = c(198, 201, 210, 185, 204, 156, 167, 197, 220, 186),
  one = c(194, 203, 200, 183, 200, 153, 166, 197, 215, 184),
  two = c(191, 200, 192, 180, 195, 150, 167, 195, 209, 179),
  three = c(188, 196, 188, 178, 191, 145, 166, 192, 205, 175)
)
rownames(dtf) <- LETTERS[1:10]
k <- ncol(dtf)
n <- nrow(dtf)
```
Calculate the Friedman test statistic:
```{r}
# Rank of each group
dtf_rank <- t(apply(dtf, MARGIN = 1, rank))
dtf_rank
# Method-1 of Chi-square value
rank_sum <- colSums(dtf_rank)
chi_sq <- 12 / (n * k * (k + 1)) * sum(rank_sum ^ 2) - 3 * n * (k + 1)
# Method-2 of Chi-square value
rank_mean <- colMeans(dtf_rank)
rank_overall <- mean(1:k)
ssb <- n * sum((rank_mean - rank_overall) ^ 2)
chi_sq <- 12 * ssb / (k * (k + 1))
chi_sq
# Critical point
chi_critical <- qchisq(0.05, df = k - 1, lower.tail = FALSE)
chi_critical
# P-value
p_value <- pchisq(chi_sq, df = k - 1, lower.tail = FALSE)
p_value
# One step
friedman.test(as.matrix(dtf))
```
<span style="color:gray">Tips: the sample is too small, so the results calculated manually may different from those using the R function which uses more accurate mathematical methods to calculate the value of the statistic.</span>

# <span style="color:gray; font-family:Microsoft JhengHei;">**13**</span> **Multiple linear regression**

## <span style="color:gray; font-family:Microsoft JhengHei;">**13.1**</span> **Definition**

- **Linear Regression:** A statistical method to determine the independent quantitative relationship between one or more predictor variables and one dependent variable.
  - **Simple Linear Regression:** Only <mark>one</mark> predictor variable.
  - **Multiple linear regression:** <mark>Two or more</mark> predictor variables.
- **Selection** of predictor variables:
  - The predictor variables must have a significant influence on the dependent variable and show a close linear correlation.
  - The predictor variables should be mutually exclusive. The degree of correlation between independent variables should not be higher than the degree of correlation between independent variables and dependent variables.
  - The predictor variables should have complete statistical data, and their predicted values can be easily determined.
- **Model:** $$\begin{array}{*{20}{l}}
{{y_1} = {\beta _0} + {\beta _1}{X_{11}} + {\beta _2}{X_{12}}... + {\beta _k}{X_{1k}} + {\varepsilon _1}}\\
{{y_2} = {\beta _0} + {\beta _1}{X_{21}} + {\beta _2}{X_{22}}... + {\beta _k}{X_{2k}} + {\varepsilon _2}}\\
{....}\\
{y_i} = {\beta _0} + {\beta _1}{X_{i1}} + {\beta _2}{X_{i2}}... + {\beta _k}{X_{ik}} + {\varepsilon _i}\\
....\\
y_n = {\beta _0} + {\beta _1}{X_{n1}} + {\beta _2}{X_{n2}}... + {\beta _k}{X_{nk}} + \varepsilon _n
\end{array}$$
$$\begin{array}{*{20}{l}}
{\begin{array}{*{20}{l}}
{{\bf{Y = (}}{{\bf{y}}_{\bf{1}}}{\bf{,}}{{\bf{y}}_{\bf{2}}}{\bf{,}}...{\bf{,}}{{\bf{y}}_{\bf{n}}}{{\bf{)}}^{\bf{T}}}}\\
{{\bf{\beta  = (}}{{\bf{\beta }}_{\bf{1}}}{\bf{,}}{{\bf{\beta }}_{\bf{2}}}{\bf{,}}...{\bf{,}}{{\bf{\beta }}_{\bf{k}}}{{\bf{)}}^{\bf{T}}}}
\end{array}}\\
{{\bf{\varepsilon  = (}}{{\bf{\varepsilon }}_{\bf{1}}}{\bf{,}}{{\bf{\varepsilon }}_{\bf{2}}}{\bf{,}}...{\bf{,}}{{\bf{\varepsilon }}_{\bf{n}}}{{\bf{)}}^{\bf{T}}}}
\end{array}$$
$${\bf{X}} = \left( {\begin{array}{*{20}{c}}
1 & {X_{11}}& {X_{12}} & \ldots &{{X_{1k}}}\\
  \vdots&\vdots &\vdots& \ddots & \vdots \\
1& {X_{n1}}&{X_{n2}}&\cdots&{{X_{nk}}}
\end{array}} \right)$$
$$\bf{Y = X\beta  + \varepsilon }$$

  - $X$: predictor variable
  - $y$: dependent variable
  - $\beta_k$: regression coefficient
  - $\epsilon$: effect of other random factors.
  - The least-squares estimator of $\beta$: $$\bf{\beta  = (X'X)^{-1}X'Y}$$
  - The least square estimation of random error variance: $${\sigma ^2}{\rm{ = }}\frac{{{e^T}e}}{{n - k - 1}}$$
- Workflow:
  - Fit the model by `lm()`
  - Diagnose the model by `summary()`
  - Optimize the model by `step()`

## <span style="color:gray; font-family:Microsoft JhengHei;">**13.2**</span> **Visualization**

```{r}
# Import the dataset which is about the US Crime Data
data(usc, package = "ACSWR")
```
```{r, fig.width = 10, fig.height = 8, fig.align='center'}
# Draw the pair plot to visualize the correlation
## pairs(usc)
GGally::ggpairs(usc)
# From the plot above, we can calculate that there have high correlation coefficient between Ex0 and Ex1
cor(usc$Ex0,usc$Ex1)
# Show the correlation coefficient directly
ucor <- cor(usc)
ucor
# Draw the correlation plot divided by two part (numerical & graphic)
corrplot::corrplot(ucor, order = 'AOE', type = "upper", method = "number")
corrplot::corrplot(ucor, order = "AOE", type = "lower", method = "ell", 
                   diag = FALSE, tl.pos = "n", cl.pos = "n", add = TRUE)
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**13.3**</span> **Fit the model**

### <span style="color:gray; font-family:Microsoft JhengHei;">**13.3.1**</span> **Analysis**
```{r}
# Do linear regression
crime_rate_lm <- lm(R ~ ., data = usc)
summary(crime_rate_lm)
# Computes confidence intervals for each parameters in this linear regression model
confint(crime_rate_lm)
# Compute analysis of variance tables for this linear regression model
anova(crime_rate_lm)
```

### <span style="color:gray; font-family:Microsoft JhengHei;">**13.3.2**</span> **Conclusion**

- The intercept terms, Age, ED, U2, and X are significant variables to explain the crime rate. The 95% confidence intervals also confirm it.
- The model is significant: $p < 0.05$, adjusted $R^2 = 0.6783$.

### <span style="color:gray; font-family:Microsoft JhengHei;">**13.3.3**</span> **The difference between $R^2$ and Adj-$R^2$**

```{r}
get_R2 <- function(x, method){
  round(summary(lm(usc$R ~ as.matrix(usc[, 2:x])))[[method]], 3)
}
dtf_R2 <- data.frame(n = 1:13,
                     R2 = sapply(2:14, get_R2, method = 'r.squared'),
                     AdjR2 = sapply(2:14, get_R2, method = 'adj.r.squared'))
library(ggplot2)
library(tidyr)
dtf_R2 |> 
  pivot_longer(-n) |> 
  ggplot() + 
  geom_point(aes(n, value, col = name)) + 
  geom_line(aes(n, value, col = name))
```

## <span style="color:gray; font-family:Microsoft JhengHei;">**13.4**</span> **Evaluate & improve the model**

### <span style="color:gray; font-family:Microsoft JhengHei;">**13.4.1**</span> **Issue**

<mark>**Multicollinearity:**</mark>

- multi-: multiple columns
- col-: relationship
- linearity: linear

**Problems:**

1. Imprecise estimates of $\beta$
2. The t-tests may fail to reveal significant factors
3. Missing importance of predictors

```{r, fig.width = 6, fig.height = 4, fig.align='center'}
# Single linear regression
data(Euphorbiaceae, package = 'gpk')
dtf <- data.frame(x1 = Euphorbiaceae$GBH,
                  y = Euphorbiaceae$Height)
plot(dtf)
lm(y ~ x1, data = dtf) |> summary()
# Multiply linear regression
dtf$x2 <- jitter(dtf$x)  # Add some random error to data
pairs(dtf)
lm(y ~ x1 + x2, data = dtf) |> summary()
```
<a id="Variance inflation factor (VIF)"></a>

### <span style="color:gray; font-family:Microsoft JhengHei;">**13.4.2**</span> **Variance inflation factor (VIF)**

$$\mathrm{VIF}_j = \frac{1}{1-R_j^2}$$

- $\mathrm{VIF}_j$**:** $VIF$ for the $j$-th variable
- $R^2_j$**:** $R^2$ from the regression of the $j$-th explanatory variable on the remaining explanatory variables.

Remove the `R` from usc dataset:
```{r}
uscrimewor <- usc[, -1]
names(uscrimewor)
```

Calculate the $VIF$:
```{r}
# Compute the VIF for Ex0
usc_lm_Ex0 <- summary(lm(Ex0  ~ ., data = uscrimewor))
1 / (1 - usc_lm_Ex0$r.squared)
# Compute all of the VIF
faraway::vif(uscrimewor)
```

<mark>**Filter:** $VIF > 10$ ($R^2 > 0.9$)</mark><br><br>
Drop the variable with highest $VIF$ and then update the model until all $VIF \leq 10$:
```{r}
uscrimewor2 <- uscrimewor[, c('Age','S','Ed','Ex0','LF','M','N','NW','U1','U2','W','X')]
faraway::vif(uscrimewor2)
crime_rate_lm2 <- lm(R ~ Age + S + Ed + Ex0 + LF + M + N + NW + U1 + U2 + W + X, usc)
summary(crime_rate_lm2)
```

**Conclusion:** The 12 explanatory variables account for 76.7% of the variability in the crime rates of the 47 states.

### <span style="color:gray; font-family:Microsoft JhengHei;">**13.4.3**</span> **Eigen System Analysis**

```{r}
uscrimewor <- as.matrix(uscrimewor)
usc_stan <- scale(uscrimewor)
x1x_stan <- t(usc_stan) %*% usc_stan
usc_eigen <- eigen(x1x_stan)
usc_kappa <- max(usc_eigen$values) / min(usc_eigen$values)  # Condition number k
```

- $k$ < 100: no multicollinearity
- 100 < $k$ < 1000: moderate multicollinearity
- $k$ > 1000: severe multicollinearity

```{r}
usec_index <- max(usc_eigen$values) / usc_eigen$values  # Condition indices
usc_eigen$vectors[, usec_index > 1000]
```
We still don't know which one should be remove, so we'd better use [Variance inflation factor (VIF)](#Variance inflation factor (VIF)).

## <span style="color:gray; font-family:Microsoft JhengHei;">**13.5**</span> **Simply the model**

### <span style="color:gray; font-family:Microsoft JhengHei;">**13.5.1**</span> **Backward selection**

1. Start with all the predictors in the model.
2. Remove the predictor with highest p-value greater than $\alpha$ (= 0.05 usually).
3. Refit the model and go to 2.
4. Stop when all p-values are less than $\alpha$.

```{r}
# This function is used to calculate the coefficients of the linear regression model and sort them from largest to smallest by P-value
get_p <- function(model){
  smry2 <- data.frame(summary(model)$coefficients)
  smry2[order(-smry2$Pr...t..), ]
}
get_p(crime_rate_lm2)
# Update-1
crime_rate_lm3 <- update(crime_rate_lm2, . ~ . - NW)
get_p(crime_rate_lm3)
# Update-2
crime_rate_lm4 <- update(crime_rate_lm3, . ~ . - LF)
get_p(crime_rate_lm4)
# Update-3
crime_rate_lm5 <- update(crime_rate_lm4,.~.-N)
get_p(crime_rate_lm5)
# Update-4
crime_rate_lm6 <- update(crime_rate_lm5,.~.-S)
get_p(crime_rate_lm6)
# Update-5
crime_rate_lm7 <- update(crime_rate_lm6,.~.-M)
get_p(crime_rate_lm7)
# Update-6
crime_rate_lm8 <- update(crime_rate_lm7,.~.-U1)
get_p(crime_rate_lm8)
# Update-7
crime_rate_lm9 <- update(crime_rate_lm8,.~.-W)
get_p(crime_rate_lm9)
# Final summary
summary(crime_rate_lm9)
```

### <span style="color:gray; font-family:Microsoft JhengHei;">**13.5.2**</span> **Forward selection**

1. Start with no variables in the model.
2. For all predictors not in the model, check their p-value if they are added to the model. Choose the one with lowest p-value less than alpha critical.
3. Continue until no new predictors can be added.

### <span style="color:gray; font-family:Microsoft JhengHei;">**13.5.3**</span> **AIC selection**

Akaike Information Criteria (AIC) **principle**: <mark>big -> bad</mark>
```{r}
crime_rate_aic <- step(crime_rate_lm, direction = "both")
summary(crime_rate_aic)
```

### <span style="color:gray; font-family:Microsoft JhengHei;">**13.5.4**</span> **Result**

**According to the backward selection:**
```{r}
equatiomatic::extract_eq(crime_rate_lm9, use_coefs = TRUE)
stargazer::stargazer(crime_rate_lm9, type = 'text')
```
The model shows that the variables Age, Ed, Ex0, U2, and X explain 73% of the variability in the crime rates.<br>

**According to the AIC selection:**
```{r}
equatiomatic::extract_eq(crime_rate_aic, use_coefs = TRUE)
stargazer::stargazer(crime_rate_aic, type = 'text')
```
The model shows that the variables Age, Ed, Ex0, U2, W, and X explain 75% of the variability in the crime rates.

# <span style="color:gray; font-family:Microsoft JhengHei;">**14**</span> **Logistic regression**

<span style="color:#008888">To be continue...</span>

# <span style="color:gray; font-family:Microsoft JhengHei;">**15**</span> **Poisson regression**

<span style="color:#008888">To be continue...</span>

# <span style="color:gray; font-family:Microsoft JhengHei;">**16**</span> **Non-linear regression**

<span style="color:#008888">To be continue...</span>

# <span style="color:gray; font-family:Microsoft JhengHei;">**17**</span> **Principal component analysis**

<span style="color:#008888">To be continue...</span>

# <span style="color:gray; font-family:Microsoft JhengHei;">**18**</span> **Cluster analysis**

<span style="color:#008888">To be continue...</span>

# **[SessionInfo:]{style="color:green"}**

```{r, echo=FALSE}
sessionInfo()
```